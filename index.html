<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Triple Storage for Random-Access Versioned Querying of RDF Archives</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css">
  <link rel="stylesheet" media="print"  href="styles/print.css">
  <link href="https://fonts.googleapis.com/css?family=Ubuntu+Mono" rel="stylesheet" />
  <link href="https://dokie.li/media/css/do.css" media="all" rel="stylesheet" />
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" media="all" rel="stylesheet" />
  <script src="https://dokie.li/scripts/simplerdf.js"></script>
  <script src="https://dokie.li/scripts/medium-editor.min.js"></script>
  <script src="https://dokie.li/scripts/medium-editor-tables.min.js"></script>
  <script src="https://dokie.li/scripts/do.js"></script>
</head>

<body prefix="cc: https://creativecommons.org/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# opmw: http://www.opmw.org/ontology/">
  <header>
  <h1 id="triple-storage-for-random-access-versionedquerying-of-rdf-archives">Triple Storage for Random-Access Versioned Querying of <span class='abbreviation' title='Resource Description Framework'>RDF</span> Archives</h1>
  <div id="repeating-title">Triple Storage for Random-Access Versioned Querying of <span class='abbreviation' title='Resource Description Framework'>RDF</span> Archives</div>

  <ul id="authors">
    <li><a href="http://www.rubensworks.net/" typeof="http://xmlns.com/foaf/0.1/Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a></li>
    <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/miel_vander_sande">Miel Vander Sande</a></li>
    <li><a href="#" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/joachim_van_herwegen">Joachim Van Herwegen</a></li>
    <li><a href="https://www.ugent.be/ea/idlab/en/members/erik-mannens.htm" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://data.verborgh.org/people/erik_mannens">Erik Mannens</a></li>
    <li><a href="https://ruben.verborgh.org/" typeof="http://xmlns.com/foaf/0.1/Person" resource="https://ruben.verborgh.org/profile/#me">Ruben Verborgh</a></li>
  </ul>

  <ul id="affiliations">
    <li id="idlab">IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec</li>
  </ul>

  <section class="context">
    <h2 id="in-reply-to">In reply to</h2>
    <ul>
      <li><a href="https://linkedresearch.org/calls" rel="as:inReplyTo">Call for Linked Research</a></li>
      <li><a href="https://journalofwebsemantics.blogspot.com/2017/06/cfp-special-issue-on-managing-evolution.html" rel="as:inReplyTo">JWS Special Issue on Managing the Evolution and Preservation of the Data Web</a> (<em>Accepted for publication</em>)</li>
    </ul>
  </section>
  <section class="actions">
    <h2 id="notifications-and-annotations">Notifications and annotations</h2>
    <ul>
      <li><a href="https://pod.linkedsoftwaredependencies.org/inbox/rdfostrich.github.io/article-jws2018-ostrich/" rel="ldp:inbox">notification inbox</a></li>
      <li><a href="https://pod.linkedsoftwaredependencies.org/annotation/rdfostrich.github.io/article-jws2018-ostrich/" rel="oa:annotationService">annotation service</a></li>
    </ul>
  </section>
</header>


<div id="content">
  <section id="abstract">
    <h2>Abstract</h2>
    <!-- Context      -->
    <p>When publishing Linked Open Datasets on the Web,
most attention is typically directed to their latest version.
Nevertheless, useful information is present in or between previous versions.
<!-- Need         -->
In order to exploit this historical information in dataset analysis,
we can maintain history in <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives.
Existing approaches either require much storage space,
or they expose an insufficiently expressive or efficient interface
with respect to querying demands.
<!-- Task         -->
In this article, we introduce an <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive indexing technique that is able to store datasets
with a low storage overhead,
by compressing consecutive versions and adding metadata for reducing lookup times.
<!-- Object       -->
We introduce algorithms based on this technique for efficiently evaluating
queries <em>at</em> a certain version, <em>between</em> any two versions, and <em>for</em> versions.
Using the <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving benchmark,
we evaluate our implementation, called <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>.
<!-- Findings     -->
Results show that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> introduces a new trade-off regarding storage space, ingestion time, and querying efficiency.
By processing and storing more metadata during ingestion time,
it significantly lowers the average lookup time for versioning queries.
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> performs better for many smaller dataset versions
than for few larger dataset versions.
Furthermore, it enables efficient offsets in query result streams,
which facilitates random access in results.
<!-- Conclusion   -->
Our storage technique reduces query evaluation time for versioned queries
through a preprocessing step during ingestion,
which only in some cases increases storage space when compared to other approaches.
This allows data owners to store and query multiple versions of their dataset efficiently,
<!-- Perspectives -->
lowering the barrier to historical dataset publication and analysis.</p>

    <p><span id="keywords"><span class="title">Keywords:</span> Linked Data, <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving, Semantic Data Versioning, storage, indexing</span></p>

  </section>


<div class="double-column">

<main>
  <section id="introduction">
        <h2>Introduction</h2>

        <p>In the area of data analysis,
there is an ongoing need for maintaining the history of datasets.
Such archives can be used for looking up data at certain points in time,
for requesting evolving changes,
or for checking the temporal validity of these data <span class="references">[<a href="#ref-1">1</a>]</span>.
With the continuously increasing number of Linked Open Datasets <span class="references">[<a href="#ref-2">2</a>]</span>,
archiving has become an issue for <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><span class='abbreviation' title='Resource Description Framework'>RDF</span></a> <span class="references">[<a href="#ref-3">3</a>]</span> data as well.
While the <span class='abbreviation' title='Resource Description Framework'>RDF</span> data model itself is atemporal, Linked Datasets typically change over time <span class="references">[<a href="#ref-4">4</a>]</span> on
dataset, schema, and/or instance level <span class="references">[<a href="#ref-5">5</a>]</span>.
Such changes can include additions,
modifications, or deletions of complete datasets, ontologies, and separate facts.
While some evolving datasets, such as DBpedia <span class="references">[<a href="#ref-6">6</a>]</span>,
are published as separate dumps per version,
more direct and efficient access to prior versions is desired.</p>

        <p>Consequently,
<span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving systems emerged that, for instance, support query engines that use the standard <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query language</a> <span class="references">[<a href="#ref-7">7</a>]</span>.
In 2015, however, <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">a survey on archiving Linked Open Data</a> <span class="references">[<a href="#ref-1">1</a>]</span> illustrated the need for improved versioning capabilities,
as current approaches have scalability issues at Web-scale.
They either perform well for versioned query evaluation—at the cost of large storage space requirements—or
require less storage space—at the cost of slower query evaluation.
Furthermore, no existing solution performs well for all versioned query types, namely querying <em>at</em>, <em>between</em>, and <em>for</em> different versions.
An efficient <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive solution should have a scalable <em>storage model</em>,
efficient <em>compression</em>, and <em>indexing methods</em> that enable expressive versioned querying <span class="references">[<a href="#ref-1">1</a>]</span>.</p>

        <p>In this article,
we argue that supporting both <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving and <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> at once is difficult to scale due to their combined complexity.
Instead, we propose an elementary but efficient versioned <em>triple pattern</em> index.
Since triple patterns are the basic element of <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span>,
such indexes can serve as an entry point for query engines.
Our solution is applicable as:
(a) an alternative index with efficient triple-pattern-based access for existing engines, in order to improve the efficiency of more expressive <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries; and
(b) a data source for the Web-friendly <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/doi:10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">Triple Pattern Fragments</a></span> <span class="references">[<a href="#ref-8">8</a>]</span> (<span class='abbreviation' title='Triple Pattern Fragments'>TPF</span>) interface, i.e.,
a Web API that provides access to <span class='abbreviation' title='Resource Description Framework'>RDF</span> datasets by triple pattern and partitions the results in pages.
We focus on the performance-critical features of <em>stream-based results</em>, query result <em>offsets</em>, and <em>cardinality estimation</em>.
Stream-based results allow more memory-efficient processing when query results are plentiful.
The capability to efficiently offset (and limit) a large stream reduces processing time if only a subset is needed.
Cardinality estimation is essential for efficient <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/doi:10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf">query planning</a></span> <span class="references">[<a href="#ref-8">8</a>, <a href="#ref-9">9</a>]</span> in many query engines.</p>

        <p>Concretely,
this work introduces a storage technique with the following contributions:</p>

        <ul>
          <li>a scalable versioned and compressed <span class='abbreviation' title='Resource Description Framework'>RDF</span> <em>index</em> with <em>offset</em> support and result <em>streaming</em>;</li>
          <li>efficient <em>query algorithms</em> to evaluate triple pattern queries and perform cardinality estimation <em>at</em>, <em>between</em>, and <em>for</em> different versions, with optional <em>offsets</em>;</li>
          <li>an open-source <em>implementation</em> of this approach called <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>;</li>
          <li>an extensive <em>evaluation</em> of <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> compared to other approaches using an existing <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving benchmark.</li>
        </ul>

        <p>The main novelty of this work is the combination of efficient offset-enabled queries over a new index structure for <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives.
We do not aim to compete with existing versioned <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> engines—full access to the language can instead be leveraged by different engines,
or by using alternative <span class='abbreviation' title='Resource Description Framework'>RDF</span> publication and querying methods such as the <span class='abbreviation' title='Hypertext Transfer Protocol'>HTTP</span> interface-based <span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> approach.
Optional versioning capabilities are possible for <span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> by using <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><span class='abbreviation' title='Versioned Triple Pattern Fragments'>VTPF</span></a> <span class="references">[<a href="#ref-10">10</a>]</span>,
or <a property="schema:citation http://purl.org/spar/cito/cites" href="http://linkeddatafragments.org/publications/jod2017.pdf">datetime content-negotiation</a> <span class="references">[<a href="#ref-11">11</a>]</span> through Memento <span class="references">[<a href="#ref-12">12</a>]</span>.</p>

        <p>This article is structured as follows.
In the following section, we start by introducing the related work and our problem statement in <a href="#problem-statement">Section 3</a>.
Next, in <a href="#fundamentals">Section 4</a>, we introduce the basic concepts of our approach,
followed by our storage approach in <a href="#storage">Section 5</a>, our ingestion algorithms in <a href="#ingestions">Section 6</a>,
and the accompanying querying algorithms in <a href="#querying">Section 7</a>.
After that, we present and discuss the evaluation of our implementation in <a href="#evaluation">Section 8</a>.
Finally, we present our conclusions in <a href="#conclusions">Section 9</a>.</p>

      </section>

  <section id="related-work">
        <h2>Related Work</h2>

        <p>In this section, we discuss existing solutions and techniques for indexing and compression in <span class='abbreviation' title='Resource Description Framework'>RDF</span> storage, without archiving support.
Then, we compare different <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving solutions.
Finally, we discuss suitable benchmarks and different query types for <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives.
This section does not contain an exhaustive list of all relevant solutions and techniques,
instead, only those that are most relevant to this work are mentioned.</p>

        <h3 id="general-rdf-indexing-and-compression">General <span class='abbreviation' title='Resource Description Framework'>RDF</span> Indexing and Compression</h3>

        <p><span class='abbreviation' title='Resource Description Framework'>RDF</span> storage systems typically use indexing and compression techniques
for reducing query times and storage space.
These systems can either be based on existing database technologies,
such as relational databases <span class="references">[<a href="#ref-13">13</a>]</span> or document stores <span class="references">[<a href="#ref-14">14</a>]</span>,
or on techniques tailored to <span class='abbreviation' title='Resource Description Framework'>RDF</span>.
These technologies can even be combined, such as approaches that detect <em>emergent schemas</em> <span class="references">[<a href="#ref-15">15</a>, <a href="#ref-16">16</a>]</span>
in <span class='abbreviation' title='Resource Description Framework'>RDF</span> datasets, which allow parts of the data to be stored in relational databases
in order to increase compression and improve the efficiency of query evaluation.
These emergent schemas are recently being exploited as <em>characteristics sets</em>
in native <span class='abbreviation' title='Resource Description Framework'>RDF</span> approaches <span class="references">[<a href="#ref-17">17</a>, <a href="#ref-18">18</a>]</span>.
For the remainder of this article, we focus the <span class='abbreviation' title='Resource Description Framework'>RDF</span>-specific techniques that have direct relevance to our approach.</p>

        <p><span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span> <span class="references">[<a href="#ref-9">9</a>]</span> is an <span class='abbreviation' title='Resource Description Framework'>RDF</span> storage technique that is based
on a clustered B+Tree with 18 indexes in which triples are sorted lexicographically.
Given that a triple consists of
a subject (S), predicate (P) and object (O),
it includes six indexes for different triple component orders (SPO, SOP, OSP, OPS, PSO and POS),
six aggregated indexes (SP, SO, PS, PO, OS, and OP),
and three one-valued indexes (S, P, and O).
A dictionary is used to compress common triple components.
When evaluating <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries, optimal indexes can be selected based on the query’s triple patterns.
Furthermore, the store allows update operations.
In our storage approach, we will reuse the concept of multiple indexes
and encoding triple components in a dictionary.</p>

        <p>Hexastore <span class="references">[<a href="#ref-19">19</a>]</span> is a similar approach as it uses six different sorted lists,
one for each possible triple component order.
Also, it uses dictionary encoding to compress common triple components.
An alternative is Triplebit <span class="references">[<a href="#ref-20">20</a>]</span>, which is based on a two-dimensional storage matrix.
Columns correspond to predicates, and rows to subjects and objects.
This sparse matrix is compressed and dictionary-encoded to reduce storage requirements.
Furthermore, it uses auxiliary index structures to improve index selection during query evaluation.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://arxiv.org/pdf/1105.4004.pdf">K2-Triples</a> <span class="references">[<a href="#ref-21">21</a>]</span> is another <span class='abbreviation' title='Resource Description Framework'>RDF</span> storage technique that uses <em>k2-tree</em>
structures to the data, which results in high compression rates.
These structures allow <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries to be evaluated in memory without decompressing the structures.</p>

        <p><span class='abbreviation' title=''>RDFCSA</span> <span class="references">[<a href="#ref-22">22</a>]</span> is a compact <span class='abbreviation' title='Resource Description Framework'>RDF</span> storage technique.
It is a <em>self-index</em> that stores the data together with its index, which results in less storage space than raw storage.
Furthermore, it is built on the concept of <em>compressed suffix arrays</em>,
which compresses text while still allowing efficient pattern-based search on it.
<span class='abbreviation' title=''>RDFCSA</span> requires about twice the storage space compared to K2-Triples, but it is faster for most queries.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><span class='abbreviation' title='Header Dictionary Triples'>HDT</span></a> <span class="references">[<a href="#ref-23">23</a>]</span> is a binary <span class='abbreviation' title='Resource Description Framework'>RDF</span> representation that is highly compressed
and provides indexing structures that enable efficient querying.
It consists of three main components:</p>

        <dl>
          <dt>Header</dt>
          <dd>metadata describing the dataset</dd>
          <dt>Dictionary</dt>
          <dd>mapping between triple components and unique IDs for reducing storage requirements of triples</dd>
          <dt>Storage</dt>
          <dd>actual triples based on the IDs of the triple components</dd>
        </dl>

        <p>The dictionary component encodes triple components in four subsets.
The first subset consists of triple components that exist both as subject and objects.
The second and third subset respectively consists of the non-common subject and object component.
The last subset consists of the predicate components.
The storage part encodes triple components using the dictionary,
compacts the triples in a sorted predicate and object adjacency list,
and stores these adjacency list in a bitmap structure that efficiently
indicates the borders of these consecutive adjacency list.
By default, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> only stores triples in the SPO-order.
When querying is required, enhanced triple indexes are constructed
to allow any triple pattern to be resolved efficiently based on the <span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span> <span class="references">[<a href="#ref-24">24</a>]</span> approach.
<span class='abbreviation' title='Header Dictionary Triples'>HDT</span> archives are read-only, which leads to high efficiency and compressibility,
but makes them unsuitable for cases where datasets change frequently.
Its fast triple pattern queries and high compression rate make it
an appropriate backend storage method for <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/doi:10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><span class='abbreviation' title='Triple Pattern Fragments'>TPF</span></a></span> <span class="references">[<a href="#ref-8">8</a>]</span> servers.
Approaches like LOD Laundromat <span class="references">[<a href="#ref-25">25</a>]</span> combine <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> and <span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> for hosting and publishing
650K+ Linked Datasets containing 38B+ triples, proving its usefulness at large scale.
Because of these reasons, we will reuse <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> snapshots as part of our storage solution.</p>

        <h3 id="related-work-archiving"><span class='abbreviation' title='Resource Description Framework'>RDF</span> Archiving</h3>

        <p>Linked Open Datasets typically change over time <span class="references">[<a href="#ref-4">4</a>]</span>,
creating a need for <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">maintaining the history of the datasets</a> <span class="references">[<a href="#ref-1">1</a>]</span>.
Hence, <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving has been an active area of research over the last couple of years.
In the domain of non-<span class='abbreviation' title='Resource Description Framework'>RDF</span> graph databases, several graph database extensions exist.
These extensions are either <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/datablend/fluxgraph">wrapper-based</a> <span class="references">[<a href="#ref-26">26</a>, <a href="#ref-27">27</a>]</span>, which leads to sub-optimal querying due to the lack of indexing,
or they are based on <a property="schema:citation http://purl.org/spar/cito/cites" href="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j">changing the graph model</a> <span class="references">[<a href="#ref-28">28</a>, <a href="#ref-29">29</a>]</span>, which complicates the writing of queries.
Furthermore, none of the existing non-<span class='abbreviation' title='Resource Description Framework'>RDF</span> graph stores offer native versioning capabilities at the time of writing.
We therefore only discuss <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving for the remainder of this section.</p>

        <p>Fernández et al. formally define an <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><em><span class='abbreviation' title='Resource Description Framework'>RDF</span> archive</em></a> <span class="references">[<a href="#ref-30">30</a>]</span> as follows:
<em>An <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive graph A is a set of version-annotated triples.</em>
Where a <em>version-annotated triple</em> <em>(s, p, o):[i]</em> is defined as <em>an <span class='abbreviation' title='Resource Description Framework'>RDF</span> triple (s, p, o) with a label i ∈ N representing the version in which this triple holds.</em>
The set of all <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><span class='abbreviation' title='Resource Description Framework'>RDF</span> triples</a> <span class="references">[<a href="#ref-3">3</a>]</span> is defined as <em>(U ∪ B) × U × (U ∪ B ∪ L)</em>,
where <em>U</em>, <em>B</em>, and <em>L</em>, respectively represent the disjoint, infinite sets of URIs, blank nodes, and literals.
Furthermore,
<em>an <span class='abbreviation' title='Resource Description Framework'>RDF</span> version of an <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive A at snapshot i is the <span class='abbreviation' title='Resource Description Framework'>RDF</span> graph A(i) = {(s, p, o)|(s, p, o):[i] ∈ A}.</em>
For the remainder of this article, we use the notation <em>V<sub>i</sub></em> to refer to the <span class='abbreviation' title='Resource Description Framework'>RDF</span> version <em>A(i)</em>.</p>

        <p>The DIACHRON data model <span class="references">[<a href="#ref-5">5</a>]</span> introduces the concept of <em>diachronic datasets</em>,
i.e., datasets that contain diachronic entities, which are semantic entities that evolve over time.
This data model formally defines a diachronic dataset as a set of dataset versions together with metadata annotations about this dataset.
Each dataset version is defined as a set of records (i.e., tuples or triples), an associated schema,
temporal information about this version and metadata specific to this version.
Domain data must be reified in order to store it in the DIACHRON model.
Due to the simplicity of <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive model compared to the domain-specific DIACHRON data model,
we will use the model of Fernández et al. for the remainder of this document.</p>

        <p>Systems for archiving Linked Open Data are categorized 
into <a property="schema:citation http://purl.org/spar/cito/cites" href="http://ceur-ws.org/Vol-1377/paper6.pdf">three non-orthogonal storage strategies</a> <span class="references">[<a href="#ref-1">1</a>]</span>:</p>

        <ul>
          <li>The <strong>Independent Copies (<span class='abbreviation' title='Independent copies'>IC</span>)</strong> approach creates separate instantiations of datasets for
each change or set of changes.</li>
          <li>The <strong>Change-Based (<span class='abbreviation' title='Change-based'>CB</span>)</strong> approach instead only stores change sets between versions.</li>
          <li>The <strong>Timestamp-Based (<span class='abbreviation' title='Timestamp-based'>TB</span>)</strong> approach stores the temporal validity of facts.</li>
        </ul>

        <p>In the following sections, we discuss several existing <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving systems, which use either pure <span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Change-based'>CB</span> or <span class='abbreviation' title='Timestamp-based'>TB</span>, or hybrid <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span>.
<a href="#rdf-archive-systems">Table 1</a> shows an overview of the discussed systems.</p>

        <figure id="rdf-archive-systems" class="table">

          <table>
            <thead>
              <tr>
                <th>Name</th>
                <th><span class='abbreviation' title='Independent copies'>IC</span></th>
                <th><span class='abbreviation' title='Change-based'>CB</span></th>
                <th><span class='abbreviation' title='Timestamp-based'>TB</span></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SemVersion <span class="references">[<a href="#ref-31">31</a>]</span></td>
                <td>✓</td>
                <td> </td>
                <td> </td>
              </tr>
              <tr>
                <td>Cassidy et. al. <span class="references">[<a href="#ref-32">32</a>]</span></td>
                <td> </td>
                <td>✓</td>
                <td> </td>
              </tr>
              <tr>
                <td>R&amp;WBase <span class="references">[<a href="#ref-33">33</a>]</span></td>
                <td> </td>
                <td>✓</td>
                <td> </td>
              </tr>
              <tr>
                <td>R43ples <span class="references">[<a href="#ref-34">34</a>]</span></td>
                <td> </td>
                <td>✓</td>
                <td> </td>
              </tr>
              <tr>
                <td>Hauptman et. al. <span class="references">[<a href="#ref-35">35</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''>X-<span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span></span> <span class="references">[<a href="#ref-36">36</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf"><span class='abbreviation' title='Resource Description Framework'>RDF</span>-TX</a> <span class="references">[<a href="#ref-37">37</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td>v-<span class='abbreviation' title=''>RDFCSA</span> <span class="references">[<a href="#ref-38">38</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td>Dydra <span class="references">[<a href="#ref-39">39</a>]</span></td>
                <td> </td>
                <td> </td>
                <td>✓</td>
              </tr>
              <tr>
                <td>TailR <span class="references">[<a href="#ref-40">40</a>]</span></td>
                <td>✓</td>
                <td>✓</td>
                <td> </td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 1:</span> Overview of <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving solutions with their corresponding storage strategy:
Individual copies (<span class='abbreviation' title='Independent copies'>IC</span>), Change-based (<span class='abbreviation' title='Change-based'>CB</span>), or Timestamp-based (<span class='abbreviation' title='Timestamp-based'>TB</span>).</p>
          </figcaption>
        </figure>

        <h4 id="independent-copies-approaches">Independent copies approaches</h4>
        <p>SemVersion <span class="references">[<a href="#ref-31">31</a>]</span> was one of the first works to look into tracking different versions of <span class='abbreviation' title='Resource Description Framework'>RDF</span> graphs.
SemVersion is based on Concurrent Versions System (<span class='abbreviation' title='Concurrent Versions System'>CVS</span>) concepts to maintain different versions of ontologies,
such as diff, branching and merging.
Their approach consists of a separation of language-specific features with ontology versioning from general features together with <span class='abbreviation' title='Resource Description Framework'>RDF</span> versioning.
Unfortunately, the implementation details on triple storage and retrieval are unknown.</p>

        <h4 id="change-based-approaches">Change-based approaches</h4>
        <p>Based on the Theory of Patches from the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://darcs.net">Darcs software management system</a> <span class="references">[<a href="#ref-41">41</a>]</span>,
Cassidy et. al. <span class="references">[<a href="#ref-32">32</a>]</span> propose to store changes to graphs as a series of patches, which makes it a <span class='abbreviation' title='Change-based'>CB</span> approach.
They describe operations on versioned graphs such as reverse, revert and merge.
An implementation of their approach is provided using the Redland python library and MySQL
by representing each patch as named graphs and serializing them in <a property="schema:citation http://purl.org/spar/cito/citesAsAuthority" href="https://www.w3.org/TR/trig/">TriG</a> <span class="references">[<a href="#ref-42">42</a>]</span>.
Furthermore, a preliminary evaluation shows that their implementation is significantly slower
than a native <span class='abbreviation' title='Resource Description Framework'>RDF</span> store. They suggest a native implementation of the approach to avoid some of the overhead.</p>

        <p>Im et. al. <span class="references">[<a href="#ref-43">43</a>]</span> propose a <span class='abbreviation' title='Change-based'>CB</span> patching system based on a relational database.
In their approach, they use a storage scheme called <em>aggregated deltas</em>
which associates the latest version with each of the previous ones.
While aggregated deltas result in fast delta queries, they introduce much storage overhead.</p>

        <p>R&amp;WBase <span class="references">[<a href="#ref-33">33</a>]</span> is a <span class='abbreviation' title='Change-based'>CB</span> versioning system that adds an additional versioning layer to existing quad-stores.
It adds the functionality of tagging, branching and merging for datasets.
The graph element is used to represent the additions and deletions of patches,
which are respectively the even and uneven graph IDs.
Queries are resolved by looking at the highest even graph number of triples.</p>

        <p>Graube et. al. introduce R43ples <span class="references">[<a href="#ref-34">34</a>]</span> which stores change sets as separate named graphs, making it a <span class='abbreviation' title='Change-based'>CB</span> system.
It supports the same versioning features as R&amp;WBase and introduces new <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> keywords for these, such as REVISION, BRANCH and TAG.
As reconstructing a version requires combining all change sets that came before,
queries at a certain version are only usable for medium-sized datasets.</p>

        <h4 id="timestamp-based-approaches">Timestamp-based approaches</h4>
        <p>Hauptman et. al. introduce a similar delta-based storage approach <span class="references">[<a href="#ref-35">35</a>]</span>
by storing each triple in a different named graph as a <span class='abbreviation' title='Timestamp-based'>TB</span> storage approach.
The identifying graph of each triple is used in a commit graph for <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query evaluation at a certain version.
Their implementation is based on Sesame <span class="references">[<a href="#ref-44">44</a>]</span> and Blazegraph <span class="references">[<a href="#ref-45">45</a>]</span> and is slower than snapshot-based approaches, but uses less disk space.</p>

        <p><span class='abbreviation' title=''>X-<span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span></span> <span class="references">[<a href="#ref-36">36</a>]</span> is an extension of <span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span> <span class="references">[<a href="#ref-9">9</a>]</span> which adds versioning support using the <span class='abbreviation' title='Timestamp-based'>TB</span> approach.
On storage-level, each triple is annotated with a creation and deletion timestamp.
This enables time-travel queries where only triples valid at the given time are returned.</p>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf"><span class='abbreviation' title='Resource Description Framework'>RDF</span>-TX</a> <span class="references">[<a href="#ref-37">37</a>]</span> is an in-memory query engine that supports a temporal <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> querying extension.
The system is based on compressed multi-version B+Trees that outperforms similar systems such as <span class='abbreviation' title=''>X-<span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span></span> in terms of querying efficiency.
The required storage space after indexing is similar to that of <span class='abbreviation' title=''>X-<span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span></span>.</p>

        <p>v-<span class='abbreviation' title=''>RDFCSA</span> <span class="references">[<a href="#ref-38">38</a>]</span> is a self-indexing <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive mechanism,
based on the <span class='abbreviation' title='Resource Description Framework'>RDF</span> self-index <span class='abbreviation' title=''>RDFCSA</span> <span class="references">[<a href="#ref-22">22</a>]</span>,
that enables versioning queries on top of compressed <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives as a <span class='abbreviation' title='Timestamp-based'>TB</span> approach.
They evaluate their approach using the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span></a> <span class="references">[<a href="#ref-30">30</a>]</span> benchmark
and show that they can reduce storage space requirements 60 times compared to raw storage.
Furthermore, they reduce query evaluation times more than an order of magnitude compared to state of the art solutions.</p>

        <p>Dydra <span class="references">[<a href="#ref-39">39</a>]</span> is an <span class='abbreviation' title='Resource Description Framework'>RDF</span> graph storage platform with dataset versioning support.
They introduce the REVISION keyword, which is similar to the GRAPH <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> keyword for referring to different dataset versions.
Their implementation is based on B+Trees that are indexed in six ways  GSPO, GPOS, GOSP, SPOG, POSG, OSPG.
Each B+Tree value indicates the revisions in which a particular quad exists, which makes it a <span class='abbreviation' title='Timestamp-based'>TB</span> approach.</p>

        <h4 id="hybrid-approaches">Hybrid approaches</h4>
        <p>TailR <span class="references">[<a href="#ref-40">40</a>]</span> is an <span class='abbreviation' title='Hypertext Transfer Protocol'>HTTP</span> archive for Linked Data pages based
on the Memento protocol <span class="references">[<a href="#ref-12">12</a>]</span> for retrieving prior versions of certain <span class='abbreviation' title='Hypertext Transfer Protocol'>HTTP</span> resources.
It is a hybrid <span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Independent copies'>IC</span> approach as it starts by storing a dataset snapshot,
after which only deltas are stored for each consecutive version, as shown in <a href="#regular-delta-chain">Fig. 1</a>.
When the chain becomes too long, or other conditions are fulfilled,
a new snapshot is created for the next version to avoid long version reconstruction times.</p>

        <p>Results show that this is an effective way of reducing version reconstruction times <span class="references">[<a href="#ref-40">40</a>]</span>,
in particular for many versions.
Within the delta chain, however, an increase in version reconstruction times can still be observed.
Furthermore, it requires more storage space than pure delta-based approaches.</p>

        <p>The authors’ implementation is based on a relational database system.
Evaluation shows that resource lookup times for any version ranges between
1 and 50 ms for 10 versions containing around 500K triples.
In total, these versions require ~64MB of storage space.</p>

        <figure id="regular-delta-chain">
<img src="img/regular-delta-chain.svg" alt="[regular delta chain]" />
<figcaption>
            <p><span class="label">Fig. 1:</span> Delta chain in which deltas are relative to the previous delta, as is done in TailR <span class="references">[<a href="#ref-40">40</a>]</span>.</p>
          </figcaption>
</figure>

        <h3 id="related-work-benchmarks"><span class='abbreviation' title='Resource Description Framework'>RDF</span> Archiving Benchmarks</h3>

        <p><a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span></a> <span class="references">[<a href="#ref-30">30</a>]</span> is a benchmark for <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive systems.
The <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> benchmark is based on three real-world datasets from different domains:</p>

        <dl>
          <dt><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></dt>
          <dd>58 weekly snapshots from the Dynamic Linked Data Observatory <span class="references">[<a href="#ref-4">4</a>]</span>. This is the main dataset from the article on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf"><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span></a> <span class="references">[<a href="#ref-30">30</a>]</span>.</dd>
          <dt><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span></dt>
          <dd>The 100 most volatile resources from DBpedia Live <span class="references">[<a href="#ref-46">46</a>]</span> over the course of three months
as three different granularities: instant, hour and day.</dd>
          <dt><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-C</span></dt>
          <dd>Dataset descriptions from the Open Data Portal Watch <span class="references">[<a href="#ref-47">47</a>]</span> project over the course of 32 weeks.</dd>
        </dl>

        <p>The 58 versions of <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> contain between 30M and 66M triples per version, with an average change ratio of 31%.
<span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> provides triple pattern queries for three different versioned query types for both result sets with a low and a high cardinality.
The queries are selected in such a way that they will be evaluated over triples of a certain dynamicity,
which requires the benchmarked systems to handle this dynamicity well.
<span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> provides a small collection of triple pattern queries corresponding to the real-world usage of DBpedia.
Finally, <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-C</span> provides 10 complex <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries that were created with the help of Open Data experts.</p>

        <p><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> provides baseline <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive implementations based on <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><span class='abbreviation' title='Header Dictionary Triples'>HDT</span></a> <span class="references">[<a href="#ref-23">23</a>]</span> and
Jena’s <span class="references">[<a href="#ref-48">48</a>]</span> <a href="https://jena.apache.org/documentation/tdb/">TDB store</a>
for the <span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Change-based'>CB</span>, and <span class='abbreviation' title='Timestamp-based'>TB</span> approaches, but also hybrid <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span> and <span class='abbreviation' title='Timestamp-based'>TB</span>/<span class='abbreviation' title='Change-based'>CB</span> approaches.
The hybrid approaches are based on snapshots followed by delta chains, as implemented by TailR <span class="references">[<a href="#ref-40">40</a>]</span>.
Due to <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> not supporting quads, the <span class='abbreviation' title='Timestamp-based'>TB</span> and <span class='abbreviation' title='Timestamp-based'>TB</span>/<span class='abbreviation' title='Change-based'>CB</span> approaches could not be implemented in the <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> baseline implementations.</p>

        <p>Results show that <span class='abbreviation' title='Independent copies'>IC</span> for both Jena and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> requires more storage space than the compressed deltas for the three datasets.
<span class='abbreviation' title='Change-based'>CB</span> results in less storage space for both approaches for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>, but not for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-C</span> because that dataset is so dynamic that
the deltas require more storage space than they would in with <span class='abbreviation' title='Independent copies'>IC</span>.
Jena-<span class='abbreviation' title='Timestamp-based'>TB</span> results in the least storage space of Jena-based approaches,
however,
it fails for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-instant because of the large amount of versions
as Jena is less efficient for many graphs.</p>

        <p>The hybrid approaches are evaluated with different delta chain lengths and expectedly show
that shorter delta chains lead to results similar to <span class='abbreviation' title='Independent copies'>IC</span>, and longer delta chains lead are similar to <span class='abbreviation' title='Change-based'>CB</span> or <span class='abbreviation' title='Timestamp-based'>TB</span>.
The queries for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> show that
<span class='abbreviation' title='Independent copies'>IC</span> results in constant evaluation times for any version,
<span class='abbreviation' title='Change-based'>CB</span> times increase for each following version,
and <span class='abbreviation' title='Timestamp-based'>TB</span> also result in constant times.
The <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches outperform Jena in all cases because of its compressed nature.
The <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span> hybrid approaches similarly show increasing evaluation times for each version,
with a drop each time a new snapshot is created.
The <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> hybrid Jena approach has slowly increasing evaluation times for each version,
but they are significantly lower than the regular <span class='abbreviation' title='Timestamp-based'>TB</span> approach.</p>

        <p>The queries of <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-C</span> currently can not be solved by the archiving strategies in a straightforward way,
but they are designed to help foster the development of future <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving solutions.
While queries of <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> are just triple pattern queries and therefore do not cover the full <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> spectrum,
they provide the basis for more complex queries, as is proven by the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/doi:10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> framework</a></span> <span class="references">[<a href="#ref-8">8</a>]</span>,
which makes them sufficient for benchmarking.</p>

        <p>EvoGen <span class="references">[<a href="#ref-49">49</a>]</span> is an <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive systems benchmark that is based on the synthetic <span class='abbreviation' title='Lehigh University Benchmark'>LUBM</span> dataset generator <span class="references">[<a href="#ref-50">50</a>]</span>.
It is an extension of the <span class='abbreviation' title='Lehigh University Benchmark'>LUBM</span> generator with additional classes and properties for introducing dataset evolution on schema-level.
EvoGen enables the user to tweak parameters of the dataset and query generation process,
for example to change the dataset dynamicity and the number of versions.</p>

        <p>While EvoGen offers more flexibility than <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> in terms of configurability.
<span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> provides real-world datasets and baseline implementations which lowers the barrier towards its usage.
Hence, we will use the <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> dataset in this work for benchmarking our system.</p>

        <h3 id="query-atoms">Query atoms</h3>

        <p>The query atoms that will be introduced in this section are based on
the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/"><span class='abbreviation' title='Resource Description Framework'>RDF</span> data model</a> <span class="references">[<a href="#ref-3">3</a>]</span> and <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/"><span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query language</a> <span class="references">[<a href="#ref-7">7</a>]</span>.
In these models, a <em>triple pattern</em> is defined as <em>(U ∪ V) × (U ∪ V) × (U ∪ L ∪ V)</em>, with <em>V</em> being the infinite set of variables.
A set of triple patterns is called a <em>Basic Graph Pattern</em>, which forms the basis of a <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query.
The evaluation of a <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query <em>Q</em> on an <span class='abbreviation' title='Resource Description Framework'>RDF</span> graph <em>G</em> containing <span class='abbreviation' title='Resource Description Framework'>RDF</span> triples,
produces a bag of solution mappings <em>[[Q]]<sub>G</sub></em>.</p>

        <p>To cover the retrieval demands in <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving,
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://semantic-web-journal.org/system/files/swj1814.pdf">five foundational query types were introduced</a> <span class="references">[<a href="#ref-30">30</a>]</span>,
which are referred to as <em>query atoms</em>:</p>

        <ol>
          <li><strong>Version materialization (<span class='abbreviation' title='Version materialization'>VM</span>)</strong> retrieves data using a query <em>Q</em> targeted at a single version <em>V<sub>i</sub></em>.
Formally: <em><span class='abbreviation' title='Version materialization'>VM</span>(Q, V<sub>i</sub>) = [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were present in the library yesterday?</em></li>
          <li><strong>Delta materialization (<span class='abbreviation' title='Delta materialization'>DM</span>)</strong> retrieves query <em>Q</em>’s result change sets between two versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em><span class='abbreviation' title='Delta materialization'>DM</span>(Q, V<sub>i</sub>, V<sub>j</sub>)=(Ω<sup>+</sup>, Ω<sup>−</sup>). With Ω<sup>+</sup> = [[Q]]<sub>V<sub>i</sub></sub> \ [[Q]]<sub>V<sub>j</sub></sub> and Ω<sup>−</sup> = [[Q]]<sub>V<sub>j</sub></sub> \ [[Q]]<sub>V<sub>i</sub></sub></em>.
Example: <em>Which books were returned or taken from the library between yesterday and now?</em></li>
          <li><strong>Version query (<span class='abbreviation' title='Version query'>VQ</span>)</strong> annotates query <em>Q</em>’s results with the versions (of <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive A) in which they are valid.
Formally: <em><span class='abbreviation' title='Version query'>VQ</span>(Q, A) = {(Ω, W) | W = {A(i) | Ω=[[Q]]<sub>A(i)</sub>, i ∈ N} ∧ Ω ≠ ∅}</em>.
Example: <em>At what times was book X present in the library?</em></li>
          <li><strong>Cross-version join (<span class='abbreviation' title='Cross-version join'>CV</span>)</strong> joins the results of two queries (<em>Q1</em> and <em>Q2</em>) between versions <em>V<sub>i</sub></em> and <em>V<sub>j</sub></em>.
Formally: <em><span class='abbreviation' title='Version materialization'>VM</span>(Q1, V<sub>i</sub>) ⨝ <span class='abbreviation' title='Version materialization'>VM</span>(Q2, V<sub>j</sub>)</em>.
Example: <em>What books were present in the library yesterday and today?</em></li>
          <li><strong>Change materialization (<span class='abbreviation' title='Change materialization'>CM</span>)</strong> returns a list of versions in which a given query <em>Q</em> produces
consecutively different results.
Formally: <em>{(i, j) | i,j ∈ ℕ, i &lt; j, <span class='abbreviation' title='Delta materialization'>DM</span>(Q, A(i), A(j)) = (Ω<sup>+</sup>, Ω<sup>−</sup>), Ω<sup>+</sup> ∪ Ω<sup>−</sup> ≠ ∅, ∄ k ∈ ℕ : i &lt; k &lt; j}</em>.
Example: <em>At what times was book X returned or taken from the library?</em></li>
        </ol>

        <p>There exists a correspondence between these query atoms
and the independent copies (<span class='abbreviation' title='Independent copies'>IC</span>), change-based (<span class='abbreviation' title='Change-based'>CB</span>), and timestamp-based (<span class='abbreviation' title='Timestamp-based'>TB</span>) storage strategies.</p>

        <p>Namely, <span class='abbreviation' title='Version materialization'>VM</span> queries are efficient in storage solutions that are based on <span class='abbreviation' title='Independent copies'>IC</span>, because there is indexing on version.
On the other hand, <span class='abbreviation' title='Independent copies'>IC</span>-based solutions may introduce a large amount of overhead in terms of storage space because each version is stored separately.
Furthermore, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries are less efficient for <span class='abbreviation' title='Independent copies'>IC</span> solutions.
That is because <span class='abbreviation' title='Delta materialization'>DM</span> queries require two fully-materialized versions to be compared on-the-fly,
and <span class='abbreviation' title='Version query'>VQ</span> requires <em>all</em> versions to be queried at the same time.</p>

        <p><span class='abbreviation' title='Delta materialization'>DM</span> queries can be efficient in <span class='abbreviation' title='Change-based'>CB</span> solutions if the query version ranges correspond to the stored delta ranges.
In all other cases, as well as for <span class='abbreviation' title='Version materialization'>VM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries, the desired versions must be materialized on-the-fly,
which will take increasingly more time for longer delta chains.
<span class='abbreviation' title='Change-based'>CB</span> solutions do however typically require less storage space than <span class='abbreviation' title='Version materialization'>VM</span> if there is sufficient overlap between each consecutive version.</p>

        <p>Finally, <span class='abbreviation' title='Version query'>VQ</span> queries perform well for <span class='abbreviation' title='Timestamp-based'>TB</span> solutions because the timestamp annotation directly corresponds to <span class='abbreviation' title='Version query'>VQ</span>’s result format.
<span class='abbreviation' title='Version materialization'>VM</span> and <span class='abbreviation' title='Delta materialization'>DM</span> queries in this case are typically less efficient than for <span class='abbreviation' title='Independent copies'>IC</span> approaches, due to the missing version index.
Furthermore, <span class='abbreviation' title='Timestamp-based'>TB</span> solutions can require less storage space compared to <span class='abbreviation' title='Version materialization'>VM</span> if the change ratio of the dataset is not too large.</p>

        <p>In summary, <span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Change-based'>CB</span> and <span class='abbreviation' title='Timestamp-based'>TB</span> approaches can perform well for certain query types, but they can be slow for others.
On the other hand, this efficiency typically comes at the cost of a large storage overhead, as is the case for <span class='abbreviation' title='Independent copies'>IC</span>-based approaches.</p>

        <p>DIACHRON QL <span class="references">[<a href="#ref-5">5</a>]</span> is a <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query language extension
based on the DIACHRON data model that provides functionality similar to these query atoms
in order to query specific versions, changesets, or all versions.</p>

      </section>

  <section id="problem-statement">
        <h2>Problem statement</h2>

        <p>As mentioned in <a href="#introduction">Section 1</a>, no <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving solutions exist that allow
efficient triple pattern querying <em>at</em>, <em>between</em>, and <em>for</em> different versions,
in combination with a scalable <em>storage model</em> and efficient <em>compression</em>.
In the context of query engines, streams are typically used to return query results,
on which offsets and limits can be applied to reduce processing time if only a subset is needed.
Offsets are used to skip a certain amount of elements,
while limits are used to restrict the number of elements to a given amount.
As such, <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving solutions should also allow query results to be returned as offsettable streams.
The ability to achieve such stream subsets is limited in existing solutions.</p>

        <p>This leads us to the following research question:
<q id="research-question">How can we store <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives to enable efficient <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> triple pattern queries with offsets?</q></p>

        <p>The focus of this article is evaluating version materialization (<span class='abbreviation' title='Version materialization'>VM</span>), delta materialization (<span class='abbreviation' title='Delta materialization'>DM</span>), and version (<span class='abbreviation' title='Version query'>VQ</span>) queries efficiently,
as <span class='abbreviation' title='Cross-version join'>CV</span> and <span class='abbreviation' title='Change materialization'>CM</span> queries can be expressed in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">terms of the other ones</a> <span class="references">[<a href="#ref-51">51</a>]</span>.
In total, our research question indentifies the following requirements:</p>

        <ul>
          <li>an efficient <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive storage technique;</li>
          <li><span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> triple pattern querying algorithms on top of this storage technique;</li>
          <li>efficient offsetting of the <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span>, and <span class='abbreviation' title='Version query'>VQ</span> query result streams.</li>
        </ul>

        <p>In this work, we lower query evaluation times by processing and storing more metadata during ingestion time.
Instead of processing metadata during every lookup, this happens only once per version.
This will increase ingestion times, but will improve the efficiency of performance-critical features
within query engines and Linked Data interfaces, such as querying with offsets.
To this end, we introduce the following hypotheses:</p>

        <ol>
          <li id="hypothesis-qualitative-querying">Our approach shows no influence of the selected versions on the querying efficiency of <span class='abbreviation' title='Version materialization'>VM</span> and <span class='abbreviation' title='Delta materialization'>DM</span> triple pattern queries.</li>
          <li id="hypothesis-qualitative-ic-storage">Our approach requires <em>less</em> storage space than state-of-the-art <span class='abbreviation' title='Independent copies'>IC</span>-based approaches.</li>
          <li id="hypothesis-qualitative-ic-querying">For our approach, querying is <em>slower</em> for <span class='abbreviation' title='Version materialization'>VM</span> and <em>equal</em> or <em>faster</em> for <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> than in state-of-the-art <span class='abbreviation' title='Independent copies'>IC</span>-based approaches.</li>
          <li id="hypothesis-qualitative-cb-storage">Our approach requires <em>more</em> storage space than state-of-the-art <span class='abbreviation' title='Change-based'>CB</span>-based approaches.</li>
          <li id="hypothesis-qualitative-cb-querying">For our approach, querying is <em>equal</em> or <em>faster</em> than in state-of-the-art <span class='abbreviation' title='Change-based'>CB</span>-based approaches.</li>
          <li id="hypothesis-qualitative-ingestion">Our approach reduces average query time compared to other non-<span class='abbreviation' title='Independent copies'>IC</span> approaches at the cost of increased ingestion time.</li>
        </ol>

      </section>

  <section id="fundamentals">
        <h2>Overview of Storage and Querying approach</h2>

        <p>In this section, we lay the groundwork for the following sections.
We introduce fundamental concepts
that are required in our storage approach and its accompanying querying algorithms,
which will be explained in <a href="#storage">Section 5</a> and <a href="#querying">Section 7</a>, respectively.</p>

        <p>To combine smart use of storage space with efficient processing of <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span>, and <span class='abbreviation' title='Version query'>VQ</span> triple pattern queries,
we employ a hybrid approach between the individual copies (<span class='abbreviation' title='Independent copies'>IC</span>), change-based (<span class='abbreviation' title='Change-based'>CB</span>), and timestamp-based (<span class='abbreviation' title='Timestamp-based'>TB</span>) storage techniques (as discussed in <a href="#related-work">Section 2</a>).
In summary, intermittent <em>fully materialized snapshots</em> are followed by <em>delta chains</em>.
Each delta chain is stored in <em>six tree-based indexes</em>, where values are dictionary-encoded and timestamped
to reduce storage requirements and lookup times.
These six indexes correspond to the combinations for storing three triple component orders
separately for additions and deletions.
The indexes for the three different triple component orders
ensure that any triple pattern query can be resolved quickly.
The additions and deletions are stored separately
because access patterns to additions and deletions in deltas differ between <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span>, and <span class='abbreviation' title='Version query'>VQ</span> queries.
To efficiently support inter-delta <span class='abbreviation' title='Delta materialization'>DM</span> queries, each addition and deletion value contains a <em>local change</em> flag
that indicates if the change is not relative to the snapshot.
Finally, in order to provide cardinality estimation for any triple pattern,
we store an additional count data structure.</p>

        <p>In the following sections, we discuss the most important distinguishing features of our approach.
We elaborate on the novel hybrid <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> storage technique that our approach is based on,
the reason for using multiple indexes,
having local change metadata,
and methods for storing addition and deletion counts.</p>

        <h3 id="snapshot-delta-chain">Snapshot and Delta Chain</h3>

        <p>Our storage technique is partially based on a hybrid <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span> approach similar to <a href="#regular-delta-chain">Fig. 1</a>.
To avoid increasing reconstruction times,
we construct the delta chain in an aggregated deltas <span class="references">[<a href="#ref-43">43</a>]</span> fashion:
each delta is <em>independent</em> of a preceding delta and relative to the closest preceding snapshot in the chain, as shown in <a href="#alternative-delta-chain">Fig. 2</a>.
Hence, for any version, reconstruction only requires at most one delta and one snapshot.
Although this does increase possible redundancies within delta chains,
due to each delta <em>inheriting</em> the changes of its preceding delta,
the overhead can be compensated with compression, which we discuss in <a href="#storage">Section 5</a>.</p>

        <figure id="alternative-delta-chain">
<img src="img/alternative-delta-chain.svg" alt="[alternative delta chain]" />
<figcaption>
            <p><span class="label">Fig. 2:</span> Delta chain in which deltas are relative to the snapshot at the start of the chain, as part of our approach.</p>
          </figcaption>
</figure>

        <h3 id="indexes">Multiple Indexes</h3>

        <p>Our storage approach consists of six different indexes that are used for separately storing additions and deletions
in three different triple component orders, namely: <code>SPO</code>, <code>POS</code> and <code>OSP</code>.
These indexes are B+Trees, thereby, the starting triple for any triple pattern can be found in logarithmic time.
Consequently, the next triples can be found by iterating through the links between each tree leaf.
<a href="#triple-pattern-index-mapping">Table 2</a> shows an overview of which triple patterns can be mapped to which index.
In contrast to other approaches <span class="references">[<a href="#ref-9">9</a>, <a href="#ref-19">19</a>]</span> that ensure certain triple orders,
we use three indexes instead of all six possible component orders,
because we only aim to reduce the iteration scope of the lookup tree for any triple pattern.
For each possible triple pattern,
we now have an index that locates the first triple component in logarithmic time,
and identifies the terminating element of the result stream without necessarily having iterate to the last value of the tree.
For some scenarios, it might be beneficial to ensure the order of triples in the result stream,
so that more efficient stream joining algorithms can be used, such as sort-merge join.
If this would be needed, <code>OPS</code>, <code>PSO</code> and <code>SOP</code> indexes could optionally be added
so that all possible triple orders would be available.</p>

        <figure id="triple-pattern-index-mapping" class="table">

          <table>
            <thead>
              <tr>
                <th>Triple pattern</th>
                <th><code>SPO</code></th>
                <th><code>SP?</code></th>
                <th><code>S?O</code></th>
                <th><code>S??</code></th>
                <th><code>?PO</code></th>
                <th><code>?P?</code></th>
                <th><code>??O</code></th>
                <th><code>???</code></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong><span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span></strong></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>OSP</code></td>
                <td><code>SPO</code></td>
                <td><code>POS</code></td>
                <td><code>POS</code></td>
                <td><code>OSP</code></td>
                <td><code>SPO</code></td>
              </tr>
              <tr>
                <td><strong><span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span></strong></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>SPO</code></td>
                <td><code>OPS</code></td>
                <td><code>PSO</code></td>
                <td><code>OPS</code></td>
                <td><code>SPO</code></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 2:</span> Overview of which triple patterns are queried inside which index in <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and <span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span>.</p>
          </figcaption>
        </figure>

        <p>Our approach could also act as a dedicated <span class='abbreviation' title='Resource Description Framework'>RDF</span> archiving solution
without (necessarily efficient) querying capabilities.
In this case, only a single index would be required, such as <code>SPO</code>, which would reduce the required storage space even further.
If querying would become required afterwards,
the auxiliary <code>OSP</code> and <code>POS</code> indexes could still be derived from this main index
during a one-time, pre-querying processing phase.</p>

        <p>This technique is similar to the <span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span> <span class="references">[<a href="#ref-24">24</a>]</span> extension for <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> that adds additional indexes to a basic <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> file
to enable faster querying for any triple pattern.
The main difference is that <span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span> uses the indexes <code>OSP</code>, <code>PSO</code> and <code>OPS</code>,
with a different triple pattern to index mapping as shown in <a href="#triple-pattern-index-mapping">Table 2</a>.
We chose our indexes in order to achieve a more balanced distribution from triple patterns to index,
which could lead to improved load balancing between indexes when queries are parallelized.
<span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span> uses <code>SPO</code> for five triple pattern groups, <code>OPS</code> for two and <code>PSO</code> for only a single group.
Our approach uses <code>SPO</code> for 4 groups, <code>POS</code> for two and <code>OSP</code> for two.
Future work is needed to evaluate the distribution for real-world queries.
Additionally, the mapping from patterns <code>S?O</code> to index <code>SPO</code> in <span class='abbreviation' title='hdt Focus on Querying'><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-FoQ</span> will lead to suboptimal query evaluation
when a large number of distinct predicates is present.</p>

        <h3 id="local-changes">Local Changes</h3>

        <p>A delta chain can contain multiple instances of the same triple,
since it could be added in one version and removed in the next.
Triples that revert a previous addition or deletion within the same delta chain, are called <em>local changes</em>,
and are important for query evaluation.
Determining the locality of changes can be costly,
thus we pre-calculate this information during ingestion time and store it for each versioned triple,
so that this does not have to happen during query-time.</p>

        <p>When evaluating version materialization queries by combining a delta with its snapshot,
all local changes should be filtered out.
For example, a triple <code>A</code> that was deleted in version 1, but re-added in version 2,
is cancelled out when materializing against version 2.
For delta materialization, these local changes should be taken into account,
because triple <code>A</code> should be marked as a deletion between versions 0 and 1,
but as an addition between versions 1 and 2.
Finally, for version queries, this information is also required
so that the version ranges for each triple can be determined.</p>

        <h3 id="addition-deletion-counts">Addition and Deletion counts</h3>

        <p>Parts of our querying algorithms depend on the ability to efficiently count
the <em>exact</em> number of additions or deletions in a delta.
Instead of naively counting triples by iterating over all of them,
we propose two separate approaches for enabling efficient addition and deletion counting in deltas.</p>

        <p>For additions, we store an additional mapping from triple pattern and version to number of additions
so that counts can happen in constant time by just looking them up in the map.
For deletions, we store additional metadata in the main deletions tree.
Both of these approaches will be further explained in <a href="#storage">Section 5</a>.</p>

      </section>

  <section id="storage">
        <h2>Hybrid Multiversion Storage Approach</h2>

        <p>In this section, we introduce our hybrid <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> storage approach for storing multiple versions of an <span class='abbreviation' title='Resource Description Framework'>RDF</span> dataset.
<a href="#storage-overview">Fig. 3</a> shows an overview of the main components.
Our approach consists of an initial dataset snapshot—stored in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><span class='abbreviation' title='Header Dictionary Triples'>HDT</span></a> <span class="references">[<a href="#ref-23">23</a>]</span>—followed by a delta chain (similar to TailR <span class="references">[<a href="#ref-40">40</a>]</span>).
The delta chain uses multiple compressed B+Trees for a <span class='abbreviation' title='Timestamp-based'>TB</span>-storage strategy (similar to Dydra <span class="references">[<a href="#ref-39">39</a>]</span>),
applies dictionary-encoding to triples, and
stores additional metadata to improve lookup times.
In this section, we discuss each component in more detail.
In the next section, we describe two ingestion algorithms based on this storage structure.</p>

        <figure id="storage-overview">
<img src="img/storage-overview.svg" alt="[storage overview]" />
<figcaption>
            <p><span class="label">Fig. 3:</span> Overview of the main components of our hybrid <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> storage approach.</p>
          </figcaption>
</figure>

        <p>Throughout this section, we will use the example <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive from <a href="#example-archive">Table 3</a>
to illustrate the different storage components with.</p>

        <figure id="example-archive" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: right">Version</th>
                <th>Triple</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: right">0</td>
                <td><code>:Bob foaf:name "Bobby"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">1</td>
                <td><code>:Alice foaf:name "Alice"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">1</td>
                <td><code>:Bob foaf:name "Bobby"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">2</td>
                <td><code>:Bob foaf:name "Bob"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">3</td>
                <td><code>:Alice foaf:name "Alice"</code></td>
              </tr>
              <tr>
                <td style="text-align: right">3</td>
                <td><code>:Bob foaf:name "Bob"</code></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 3:</span> Example of a small <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive with 4 versions.
We assume that the following URI prefixes: <code>: http://example.org</code>, <code>foaf: http://xmlns.com/foaf/0.1/</code></p>
          </figcaption>
        </figure>

        <h3 id="snapshot-storage">Snapshot storage</h3>

        <p>As mentioned before, the start of each delta chain is a fully materialized snapshot.
In order to provide sufficient efficiency for <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> querying with respect to all versions in the chain,
we assume the following requirements for the snapshot storage:</p>

        <ul>
          <li>Any triple pattern query <em>must</em> be resolvable as triple streams.</li>
          <li>Offsets <em>must</em> be applicable to the result stream of any triple pattern query.</li>
          <li>Cardinality estimation for all triple pattern queries <em>must</em> be possible.</li>
        </ul>

        <p>These requirements are needed for ensuring the efficiency of the querying algorithms that will be introduced in <a href="#querying">Section 7</a>.
For the implementation of snapshots,
existing techniques such as <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><span class='abbreviation' title='Header Dictionary Triples'>HDT</span></a> <span class="references">[<a href="#ref-23">23</a>]</span> fulfill all the requirements.
Therefore,
we do not introduce a new snapshot approach, but use <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> in our implementation.
This will be explained further in <a href="#implementation">Subsection 8.1</a>.</p>

        <h3 id="dictionary">Delta Chain Dictionary</h3>

        <p>A common technique in <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><span class='abbreviation' title='Resource Description Framework'>RDF</span> indexes</a> <span class="references">[<a href="#ref-23">23</a>, <a href="#ref-9">9</a>, <a href="#ref-20">20</a>]</span> is to use a dictionary for mapping triple components to numerical IDs.
This is done for three main reasons:
1) reduce storage space if triple components are stored multiple times;
2) reducing I/O overhead when retrieving data; and
3) simplify and optimize querying.
As our storage approach essentially stores each triple three or six times,
a dictionary can definitely reduce storage space requirements.</p>

        <p>Each delta chain consists of two dictionaries, one for the snapshot and one for the deltas.
The snapshot dictionary consists of triple components that already existed in the snapshot.
All other triple components are stored in the delta dictionary.
This dictionary is shared between the additions and deletions,
as the dictionary ignores whether or not the triple is an addition or deletion.
How this distinction is made will be explained in <a href="#delta-storage">Subsection 5.3</a>.
The snapshot dictionary can be optimized and sorted, as it will not change over time.
The delta dictionary is volatile, as each new version can introduce new mappings.</p>

        <p>During triple encoding (i.e., ingestion), the snapshot dictionary will always first be probed for existence of the triple component.
If there is a match, that ID is used for storing the delta’s triple component.
To identify the appropriate dictionary for triple decoding,
a reserved bit is used where <code>1</code> indicates snapshot dictionary
and <code>0</code> indicates the delta dictionary.
The text-based dictionary values can be compressed to reduce storage space further, as they are likely to contain many redundancies.</p>

        <p><a href="#example-delta-storage-dict">Table 4</a> contains example encodings of the triple components.</p>

        <figure id="example-delta-storage-dict" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: right"><code>:Bob</code></th>
                <th style="text-align: right"><code>foaf:name</code></th>
                <th style="text-align: right"><code>"Bobby"</code></th>
                <th style="text-align: right"><code>:Alice</code></th>
                <th style="text-align: right"><code>"Alice"</code></th>
                <th style="text-align: right"><code>"Bob"</code></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: right"><code>S0</code></td>
                <td style="text-align: right"><code>S1</code></td>
                <td style="text-align: right"><code>S2</code></td>
                <td style="text-align: right"><code>D0</code></td>
                <td style="text-align: right"><code>D1</code></td>
                <td style="text-align: right"><code>D2</code></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 4:</span> Example encoding of the triple components from <a href="#example-archive">Table 3</a>.
Instead of the reserved bit, IDs prefixed with <code>S</code> belong to the snapshot dictionary
and those prefixed with <code>D</code> belong to the delta dictionary.</p>
          </figcaption>
        </figure>

        <h3 id="delta-storage">Delta Storage</h3>

        <p>In order to cope with the newly introduced redundancies in our delta chain structure,
we introduce a delta storage method similar to the <span class='abbreviation' title='Timestamp-based'>TB</span> storage strategy,
which is able to compress redundancies within consecutive deltas.
In contrast to a regular <span class='abbreviation' title='Timestamp-based'>TB</span> approach, which stores plain timestamped triples,
we store timestamped triples annotated with a flag for addition or deletion.
An overview of this storage technique is shown in <a href="#delta-storage-overview">Fig. 4</a>,
which will be explained in detail hereafter.</p>

        <figure id="delta-storage-overview">
<img src="img/delta-storage-overview.svg" alt="[delta storage overview]" />
<figcaption>
            <p><span class="label">Fig. 4:</span> Overview of the components for storing a delta chain.
The value structure for the addition and deletion trees are indicated with the dashed nodes.</p>
          </figcaption>
</figure>

        <p>The additions and deletions of deltas require different metadata in our querying algorithms,
which will be explained in <a href="#querying">Section 7</a>.
Additions and deletions are respectively stored in separate stores,
which hold all additions and deletions from the complete delta chain.
Each store uses B+Tree data structures,
where a key corresponds to a triple and the value contains version information.
The version information consists of a mapping from version to a local change flag as mentioned in <a href="#local-changes">Subsection 4.3</a> and,
in case of deletions, also the relative position of the triple inside the delta.
Even though triples can exist in multiple deltas in the same chain,
they will only be stored once.
Each addition and deletion store uses three trees with a different triple component order (SPO, POS and OSP),
as discussed in <a href="#indexes">Subsection 4.2</a>.</p>

        <p>The relative position of each triple inside the delta to the deletion trees speeds up the process
of patching a snapshot’s triple pattern subset for any given offset.
In fact, seven relative positions are stored for each deleted triple: one for each possible triple pattern (<code>SP?</code>, <code>S?O</code>, <code>S??</code>, <code>?PO</code>, <code>?P?</code>, <code>??O</code>, <code>???</code>),
except for <code>SPO</code> since this position will always be 0 as each triple is stored only once.
This position information serves two purposes:
1) it allows the querying algorithm to exploit offset capabilities of the snapshot store
to resolve offsets for any triple pattern against any version;
and 2) it allows deletion counts for any triple pattern and version to be determined efficiently.
The use of the relative position and the local change flag during querying will be further explained in <a href="#querying">Section 7</a>.</p>

        <figure id="example-delta-storage" class="table">

          <table>
            <thead>
              <tr>
                <th>+</th>
                <th style="text-align: right">V</th>
                <th>L</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><code>D0 S1 D1</code></td>
                <td style="text-align: right">1</td>
                <td>F</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">3</td>
                <td>F</td>
              </tr>
              <tr>
                <td><code>S0 S1 D2</code></td>
                <td style="text-align: right">2</td>
                <td>F</td>
              </tr>
            </tbody>
          </table>

          <table>
            <thead>
              <tr>
                <th>-</th>
                <th style="text-align: right">V</th>
                <th>L</th>
                <th style="text-align: right"><code>SP?</code></th>
                <th style="text-align: right"><code>S?O</code></th>
                <th style="text-align: right"><code>S??</code></th>
                <th style="text-align: right"><code>?PO</code></th>
                <th style="text-align: right"><code>?P?</code></th>
                <th style="text-align: right"><code>??O</code></th>
                <th style="text-align: right"><code>???</code></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><code>D0 S1 D1</code></td>
                <td style="text-align: right">2</td>
                <td>T</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
              </tr>
              <tr>
                <td><code>S0 S1 S2</code></td>
                <td style="text-align: right">2</td>
                <td>F</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">1</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">1</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">3</td>
                <td>F</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
                <td style="text-align: right">0</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 5:</span> Addition and deletion tree contents based on the example from <a href="#example-archive">Table 3</a> using the dictionary encoding from <a href="#example-delta-storage-dict">Table 4</a>.
Column <code>+</code> and <code>-</code> respectively represent the keys of the addition and deletion trees, which contains triples based on the encoded triple components.
The remaining columns represent the values, i.e., a mapping from version (<code>V</code>) to the local change flag (<code>L</code>).
For the deletion trees, values also include the relative positions for all essential triple patterns.</p>
          </figcaption>
        </figure>

        <p><a href="#example-delta-storage">Table 5</a> represent
the addition and deletion tree contents when the triples from the example in <a href="#example-archive">Table 3</a> are stored.
The local change flag is enabled for <code>D0 S1 D1</code> in the deletions tree for version 2, as it was previously added in version 1.
The relative positions in the deletion tree for <code>S0 S1 S2</code> is not the same for versions 2 and 3,
because in version 2, the triple <code>D0 S1 D1</code> also exists as a deletion, and when sorted, this comes before <code>S0 S1 S2</code> for triple patterns <code>?P?</code> and <code>???</code>.</p>

        <h3 id="addition-counts">Addition Counts</h3>

        <p>As mentioned before in <a href="#addition-deletion-counts">Subsection 4.4</a>,
in order to make the counting of matching addition triples for any triple pattern for any version more efficient,
we propose to store an additional mapping from triple pattern and version to the number of matching additions.
Furthermore, for being able to retrieve the total number of additions across all versions,
we also propose to store this value for all triple patterns.
This mapping must be calculated during ingestion time, so that counts during lookup time for any triple pattern
at any version can be derived in constant time.
For many triples and versions, the number of possible triple patterns can become very large,
which can result in a large mapping store.
To cope with this, we propose to only store the elements where their counts are larger than a certain threshold.
Elements that are not stored will have to be counted during lookup time.
This is however not a problem for reasonably low thresholds,
because the iteration scope in our indexes can be limited efficiently, as mentioned in <a href="#addition-deletion-counts">Subsection 4.4</a>.
The count threshold introduces a trade-off between the storage requirements and the required triple counting during lookups.</p>

        <h3 id="deletion-counts">Deletion Counts</h3>

        <p>As mentioned in <a href="#delta-storage">Subsection 5.3</a>, each deletion is annotated with its relative position in all deletions for that version.
This position is exploited to perform deletion counting for any triple pattern and version.
We look up the largest possible triple (sorted alphabetically) for the given triple pattern in the deletions tree,
which can be done in logarithmic time by navigating in the tree to the largest possible match for the given triple pattern.
If this does not result in a match for the triple pattern, no matches exist for the given triple pattern, and the count is zero.
Otherwise, we take one plus the relative position of the matched deletion for the given triple pattern.
Because we have queried the largest possible triple for that triple pattern in the given version,
this will be the last deletion in the list, so this position corresponds to the total number of deletions in that case.</p>

        <p>For example, when we want to determine the deletion count for <code>? foaf:name ?</code> (encoded: <code>? S1 ?</code>) in version 2
using the deletion tree contents from <a href="#example-delta-storage">Table 5</a>,
we will find <code>S0 S1 S2</code> as largest triple in version 2.
This triple has relative position <code>1</code> for <code>?P?</code>, so the total deletion count is <code>2</code> for this pattern.
This is correct, as we have indeed two triples matching this pattern, namely <code>D0 S1 D1</code> and <code>S0 S1 S2</code>.</p>

        <h3 id="metadata">Metadata</h3>

        <p>Querying algorithms have to be able to detect the total number of versions across all delta chains.
Therefore,
we must store metadata regarding the delta chain version ranges.
Assuming that version identifiers are numerical, a mapping can be maintained from version ID to delta chain.
Additionally, a counter of the total number of versions must be maintained for when the last version must be identified.</p>

      </section>

  <section id="ingestions">
        <h2>Changeset Ingestion Algorithms</h2>

        <p>In this section, we discuss two ingestion algorithms: a memory-intensive batch algorithm and a memory-efficient streaming algorithm.
These algorithms both take a changeset—containing additions and deletions—as input,
and append it as a new version to the store.
Note that the ingested changesets are regular changesets: they are relative to one another according to <a href="#regular-delta-chain">Fig. 1</a>.
Furthermore, we assume that the ingested changesets are <em>valid</em> changesets:
they don’t contain impossible triple sequences such as a triple that is removed in two versions without having an addition in between.
During ingestion, they will be transformed to the alternative delta chain structure as shown in <a href="#alternative-delta-chain">Fig. 2</a>.
Within the scope of this article, we only discuss ingestion of deltas in a single delta chain following a snapshot.</p>

        <p>Next to ingesting the added and removed triples,
an ingestion algorithm for our storage approach must be able to calculate
the appropriate metadata for the store as discussed in <a href="#delta-storage">Subsection 5.3</a>.
More specifically, an ingestion algorithm has the following requirements:</p>
        <ul>
    <li>addition triples must be stored in all addition trees;</li>
    <li>additions and deletions must be annotated with their version;</li>
    <li>additions and deletions must be annotated with being a local change or not;</li>
    <li>deletions must be annotated with their relative position for all triple patterns.</li>
</ul>

        <h3 id="batch-ingestion">Batch Ingestion</h3>

        <p>Our first algorithm to ingest data into the store naively loads everything in memory,
and inserts the data accordingly.
The advantage of this algorithm is its simplicity and the possibility to do straightforward optimizations during ingestion.
The main disadvantage is the high memory consumption requirement for large versions.</p>

        <p>Before we discuss the actual batch ingestion algorithm,
we first introduce an in-memory changeset merging algorithm,
which is required for the batch ingestion.
<a href="#algorithm-ingestion-batch-merge">Algorithm 1</a> contains the pseudocode of this algorithm.
First, all contents of the original changeset are copied into the new changeset (line 3).
After that, we iterate over all triples of the second changeset (line 4).
If the changeset already contained the given triple (line 5), the local change flag is negated.
Otherwise, the triple is added to the new changeset, and the local change flag is set to <code>false</code> (line 9,10).
Finally, in both cases the addition flag of the triple in the new changeset is copied from the second changeset (line 12).</p>

        <figure id="algorithm-ingestion-batch-merge" class="algorithm numbered">
<pre><code>mergeChangesets(changesetOriginal, changesetIngest) {
</code><code>  changesetNew = new Changeset()
</code><code>  changesetNew.addAll(changesetOriginal)
</code><code>  for (triple : changesetIngest.getTriples()) {
</code><code>    if (changesetOriginal.contains(triple)) {
</code><code>      localChange = !changesetOriginal.isLocalChange(triple)
</code><code>      changesetNew.setLocalChange(triple, localChange)
</code><code>    } else {
</code><code>      changesetNew.add(triple)
</code><code>      changesetNew.setLocalChange(triple, false)
</code><code>    }
</code><code>    changesetNew.setAddition(triple, changesetIngest.isAddition(triple))
</code><code>  }
</code><code>  return changesetNew
</code><code>}</code></pre>
<figcaption>
            <p><span class="label">Algorithm 1:</span> In-memory changeset merging algorithm</p>
          </figcaption>
</figure>

        <p>Because our querying algorithms require the relative position of each deletion within a changeset to be stored,
we have to calculate these positions during ingestion.
We do this using the helper function <code>calculatePositions(triple)</code>.
This function depends on external mappings that persist over the duration of the ingestion phase
that map from triple to a counter for each possible triple pattern.
When this helper function is called for a certain triple,
we increment the counters for the seven possible triple patterns of the triple.
For the triple itself, we do not maintain a counter, as its value is always 1.
Finally, the function returns a mapping for the current counter values of the seven triple patterns.</p>

        <p>The batch ingestion algorithm starts by reading a complete changeset stream in-memory, sorting it in SPO order,
and encoding all triple components using the dictionary.
After that, it loads the changeset from the previous version in memory,
which is required for merging it together with the new changeset using the algorithm from <a href="#algorithm-ingestion-batch-merge">Algorithm 1</a>.
After that, we have the new changeset loaded in memory.
Now, we load each added triple into the addition trees, together with their version and local change flag.
After that, we load each deleted triple into the deletion trees
with their version, local change flag and relative positions.
These positions are calculated using <code>calculatePositions(triple)</code>.
For the sake of completeness, we included the batch algorithm in pseudo-code in <a href="#appendix-algorithms">Appendix D</a>.</p>

        <p>Even though this algorithm is straightforward,
it can require a large amount of memory for large changesets and long delta chains.
The theoretical time complexity of this algorithm is <code>O(P + N log(N))</code> (<code>O(P + N)</code> if the new changeset is already sorted),
with <code>P</code> the number of triples in the previous changeset,
and <code>N</code> the number of triples in the new changeset.</p>

        <h3 id="streaming-ingestion">Streaming Ingestion</h3>

        <p>Because of the unbounded memory requirements of the <a href="#batch-ingestion">batch ingestion algorithm</a>,
we introduce a more complex streaming ingestion algorithm.
Just like the batch algorithm, it takes a changeset stream as input,
with the additional requirement that the stream’s values must be sorted in SPO-order.
This way the algorithm can assume a consistent order and act as a sort-merge join operation.
Just as for the batch algorithm, we included this algorithm in pseudo-code in <a href="#appendix-algorithms">Appendix D</a>.</p>

        <p>In summary, the algorithm performs a sort-merge join over three streams in SPO-order:
1) the stream of <em>input</em> changeset elements that are encoded using the dictionary when each element is read,
2) the existing <em>deletions</em> over all versions
and 3) the existing <em>additions</em> over all versions.
The algorithm iterates over all streams together, until all of them are finished.
The smallest triple (string-based) over all stream heads is handled in each iteration,
and can be categorized in seven different cases where these stream heads are indicated by <em>input</em>, <em>deletion</em> and <em>addition</em>, respectively:</p>

        <ol>
<li>
            <p><strong><em>Deletion</em> is strictly smaller than both <em>input</em> and <em>addition</em>.</strong>
<br />
The current deletion is the smallest element.
The unchanged deletion information can be copied to the new version.
New relative positions must be calculated in this and all other cases where deletions are added.</p>
          </li>
<li>
            <p><strong><em>Addition</em> is strictly smaller than both <em>input</em> and <em>deletion</em>.</strong>
<br />
Similar to the previous case, the current addition is now the smallest element,
and its information can be copied to the new version.</p>
          </li>
<li>
            <p><strong><em>Input</em> is strictly smaller than both <em>addition</em> and <em>deletion</em>.</strong>
<br />
A triple is added or removed that was not present before,
so it can respectively be added as a non-local change addition or a non-local change deletion.</p>
          </li>
<li>
            <p><strong><em>Input</em> and <em>deletion</em> are equal, but strictly smaller than <em>addition</em>.</strong>
<br />
In this case, the new triple already existed in the previous version as a deletion.
If the new triple is an addition, it must be added as a local change.</p>
          </li>
<li>
            <p><strong><em>Input</em> and <em>addition</em> are equal, but strictly smaller than <em>deletion</em>.</strong>
<br />
Similar as in the previous case, the new triple now already existed as an addition.
So the triple must be deleted as a local change if the new triple is a deletion.</p>
          </li>
<li>
            <p><strong><em>Addition</em> and <em>deletion</em> are equal, but strictly smaller than <em>input</em>.</strong>
<br />
The triple existed as both an addition and deletion at some point.
In this case, we copy over the one that existed at the latest version, as it will still apply in the new version.</p>
          </li>
<li>
            <p><strong><em>Addition</em>,  <em>deletion</em>, and <em>input</em> are equal.</strong>
<br />
Finally, the triple already existed as both an addition and deletion,
and is equal to our new triple.
This means that if the triple was an addition in the previous version, it becomes a deletion, and the other way around,
and the local change flag can be inherited.</p>
          </li>
</ol>

        <p>The theoretical memory requirement for this algorithm is much lower than the <a href="#batch-ingestion">batch variant</a>.
That is because it only has to load at least three triples, i.e., the heads of each stream, in memory, instead of the complete new changeset.
Furthermore, we still need to maintain the relative position counters for the deletions in all triple patterns.
While these counters could also become large, a smart implementation could perform memory-mapping
to avoid storing everything in memory.
The lower memory requirements come at the cost of a higher logical complexity, but an equal time complexity (assuming sorted changesets).</p>

      </section>

  <section id="querying">
        <h2>Versioned Query Algorithms</h2>

        <p>In this section, we introduce algorithms for performing <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> triple pattern queries
based on the storage structure introduced in <a href="#storage">Section 5</a>.
Each of these querying algorithms are based on result streams, enabling efficient offsets and limits,
by exploiting the index structure from <a href="#storage">Section 5</a>.
Furthermore, we provide algorithms to provide count estimates for each query.</p>

        <h3 id="version-materialization">Version Materialization</h3>

        <p>Version Materialization (<span class='abbreviation' title='Version materialization'>VM</span>) is the most straightforward versioned query type,
it allows you to query against a certain dataset version.
In the following, we start by introducing our <span class='abbreviation' title='Version materialization'>VM</span> querying algorithm,
after we give a simple example of this algorithm.
After that, we prove the correctness of our <span class='abbreviation' title='Version materialization'>VM</span> algorithm and introduce a corresponding algorithm to provide count estimation for <span class='abbreviation' title='Version materialization'>VM</span> query results.</p>

        <h4 id="query">Query</h4>

        <p><a href="#algorithm-querying-vm">Algorithm 2</a> introduces an algorithm for <span class='abbreviation' title='Version materialization'>VM</span> triple pattern queries based on our storage structure.
It starts by determining the snapshot on which the given version is based (line 2).
After that, this snapshot is queried for the given triple pattern and offset.
If the given version is equal to the snapshot version, the snapshot iterator can be returned directly (line 3).
In all other cases, this snapshot offset could only be an estimation,
and the actual snapshot offset can be larger if deletions were introduced before the actual offset.</p>

        <p>Our algorithm returns a stream where triples originating from the snapshot always
come before the triples that were added in later additions.
Because of that, the mechanism for determining the correct offset in the
snapshot, additions and deletions streams can be split up into two cases.
The given offset lies within the range of either snapshot minus deletion triples or within the range of addition triples.
At this point, the additions and deletions streams are initialized to the start position for the given triple pattern and version.</p>

        <figure id="algorithm-querying-vm" class="algorithm numbered">
<pre><code>queryVm(store, tp, version, originalOffset) {
</code><code>  snapshot = store.getSnapshot(version).query(tp, originalOffset)
</code><code>  if (snapshot.getVersion() = version) {
</code><code>    return snapshot
</code><code>  }
</code><code>  
</code><code>  additions = store.getAdditionsStream(tp, version)
</code><code>  deletions = store.getDeletionStream(tp, version)
</code><code>  offset = 0
</code><code>  
</code><code>  if (originalOffset &lt; snapshot.count(tp) - deletions.exactCount(tp)) {
</code><code>    do {
</code><code>      snapshot.offset(originalOffset + offset)
</code><code>      offsetTriple = snapshot.peek()
</code><code>      deletions = store.getDeletionsStream(tp, version, offsetTriple)
</code><code>      offset = deletions.getOffset(tp)
</code><code>    } while (snapshot.getCurrentOffset() != originalOffset + offset)
</code><code>  }
</code><code>  else {
</code><code>    snapshot.offset(snapshot.count(tp))
</code><code>    additions.offset(originalOffset - snapshot.count(tp)
</code><code>        + deletions.exactCount(tp))
</code><code>  }
</code><code>  
</code><code>  return PatchedSnapshotIterator(snapshot, deletions, additions)
</code><code>}</code></pre>
<figcaption>
            <p><span class="label">Algorithm 2:</span> Version Materialization algorithm for triple patterns that produces a triple stream with an offset in a given version.</p>
          </figcaption>
</figure>

        <p>In the first case, when the offset lies within the snapshot and deletions range (line 11),
we enter a loop that converges to the actual snapshot offset based on the deletions
for the given triple pattern in the given version.
This loop starts by determining the triple at the current offset position in the snapshot (line 13, 14).
We then query the deletions tree for the given triple pattern and version (line 15),
filter out local changes, and use the snapshot triple as offset.
This triple-based offset is done by navigating through the tree to the smallest triple before or equal to the offset triple.
We store an additional offset value (line 16), which corresponds to the current numerical offset inside the deletions stream.
As long as the current snapshot offset is different from the sum of the original offset and the additional offset,
we continue iterating this loop (line 17), which will continuously increase this additional offset value.</p>

        <p>In the second case (line 19), the given offset lies within the additions range.
Now, we terminate the snapshot stream by offsetting it after its last element (line 20),
and we relatively offset the additions stream (line 21).
This offset is calculated as the original offset subtracted with the number of snapshot triples incremented with the number of deletions.</p>

        <p>Finally, we return a simple iterator starting from the three streams (line 25).
This iterator performs a sort-merge join operation that removes each triple from the snapshot that also appears in the deletion stream,
which can be done efficiently because of the consistent <code>SPO</code>-ordering.
Once the snapshot and deletion streams have finished,
the iterator will start emitting addition triples at the end of the stream.
For all streams, local changes are filtered out because locally changed triples
are cancelled out for the given version as explained in <a href="#local-changes">Subsection 4.3</a>,
so they should not be returned in materialized versions.</p>

        <h4 id="example">Example</h4>

        <p>We can use the deletion’s position in the delta as offset in the snapshot
because this position represents the number of deletions that came before that triple inside the snapshot given a consistent triple order.
<a href="#query-vm-example">Table 6</a> shows simplified storage contents where triples are represented as a single letter,
and there is only a single snapshot and delta.
In the following paragraphs, we explain the offset convergence loop of the algorithm in function of this data for different offsets.</p>

        <figure id="query-vm-example" class="table">

          <table>
            <thead>
              <tr>
                <th>Snapshot</th>
                <th>A</th>
                <th>B</th>
                <th>C</th>
                <th>D</th>
                <th>E</th>
                <th>F</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Deletions</td>
                <td> </td>
                <td>B</td>
                <td> </td>
                <td>D</td>
                <td>E</td>
                <td> </td>
              </tr>
              <tr>
                <td>Positions</td>
                <td> </td>
                <td>0</td>
                <td> </td>
                <td>1</td>
                <td>2</td>
                <td> </td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 6:</span> Simplified storage contents example where triples are represented as a single letter.
The snapshot contains six elements, and the next version contains three deletions.
Each deletion is annotated with its position.</p>
          </figcaption>
        </figure>

        <h5 id="offset-0"><em>Offset 0</em></h5>
        <p>For offset zero, the snapshot is first queried for this offset,
which results in a stream starting from <code>A</code>.
Next, the deletions are queried with offset <code>A</code>, which results in no match,
so the final snapshot stream starts from <code>A</code>.</p>

        <h5 id="offset-1"><em>Offset 1</em></h5>
        <p>For an offset of one, the snapshot stream initially starts from <code>B</code>.
After that, the deletions stream is offset to <code>B</code>, which results in a match.
The original offset (1), is increased with the position of <code>B</code> (0) and the constant 1,
which results in a new snapshot offset of 2.
We now apply this new snapshot offset.
As the snapshot offset has changed, we enter a second iteration of the loop.
Now, the head of the snapshot stream is <code>C</code>.
We offset the deletions stream to <code>C</code>, which again results in <code>B</code>.
As this offset results in the same snapshot offset,
we stop iterating and use the snapshot stream with offset 2 starting from <code>C</code>.</p>

        <h5 id="offset-2"><em>Offset 2</em></h5>
        <p>For offset 2, the snapshot stream initially starts from <code>C</code>.
After querying the deletions stream, we find <code>B</code>, with position 0.
We update the snapshot offset to 2 + 0 + 1 = 3,
which results in the snapshot stream with head <code>D</code>.
Querying the deletions stream results in <code>D</code> with position 1.
We now update the snapshot offset to 2 + 1 + 1 = 4, resulting in a stream with head <code>E</code>.
We query the deletions again, resulting in <code>E</code> with position 2.
Finally, we update the snapshot offset to 2 + 2 + 1 = 5 with stream head <code>F</code>.
Querying the deletions results in the same <code>E</code> element,
so we use this last offset in our final snapshot stream.</p>

        <h4 id="estimated-count">Estimated count</h4>

        <p>In order to provide an estimated count for <span class='abbreviation' title='Version materialization'>VM</span> triple pattern queries,
we introduce a straightforward algorithm that depends on the efficiency of the snapshot to provide count estimations for a given triple pattern.
Based on the snapshot count for a given triple pattern, the number of deletions for that version and triple pattern
are subtracted and the number of additions are added.
These last two can be resolved efficiently, as we precalculate
and store expensive addition and deletion counts as explained in <a href="#addition-counts">Subsection 5.4</a> and <a href="#deletion-counts">Subsection 5.5</a>.</p>

        <h4 id="correctness">Correctness</h4>

        <p>In this section, we provide a proof that <a href="#algorithm-querying-vm">Algorithm 2</a> results in the correct stream offset
for any given version and triple pattern. We do this by first introducing a set of notations,
followed by several lemmas and corollaries, which lead up to our final theorem proof.</p>

        <p><strong>Notations</strong>:</p>

        <p>We will make use of bracket notation to indicate lists (ordered sets):</p>

        <ul>
          <li><code>A[i]</code> is the element at position <code>i</code> from the list <code>A</code>.</li>
          <li><code>A + B</code> is the concatenation of list <code>A</code> followed by list <code>B</code>.</li>
        </ul>

        <p>Furthermore, we will use the following definitions:</p>

        <ul>
          <li><code>snapshot(tp, version)</code> is the ordered list of triples matching the given triple pattern <code>tp</code> in the corresponding snapshot, from here on shortened to <code>snapshot</code>.</li>
          <li><code>additions(version)</code> and <code>deletions(version)</code> are the corresponding ordered additions and deletions for the given version, from here on shortened to <code>additions</code> and <code>deletions</code>.</li>
          <li><code>originalOffset</code> is how much the versioned list should be shifted, from here on shortened to <code>ori</code>.</li>
          <li><code>PatchedSnapshotIterator(snapshot, deletions, additions)</code> is a function that returns the list <code>snapshot\deletions + additions</code>.</li>
        </ul>

        <p>The following definitions correspond to elements from the loop on lines 12-17:</p>

        <ul>
          <li><code>deletions(x)</code> is the ordered list <code>{d | d ∈ deletions, d ≥ x}</code>, with <code>x</code> a triple.</li>
          <li><code>offset(x) = |deletions| - |deletions(x)|</code>, with <code>x</code> a triple.</li>
          <li><code>t(i)</code> is the triple generated at line 13-14 for iteration <code>i</code>.</li>
          <li><code>off(i)</code> is the offset generated at line 16 for iteration <code>i</code>.</li>
        </ul>

        <p><strong>Lemma 1</strong>: <code>off(n) ≥ off(n-1)</code><br />
<em>Proof</em>:<br />
We prove this by induction over the iterations of the loop.
For <code>n=1</code> this follows from line 9 and <code>∀ x offset(x) ≥ 0.</code></p>

        <p>For <code>n+1</code> we know by induction that <code>off(n) ≥ off(n-1)</code>.
Since <code>snapshot</code> is ordered, <code>snapshot[ori + off(n)] ≥ snapshot[ori + off(n-1)]</code>.
From lines 13-14 follows that <code>t(n) = snapshot[ori + off(n-1)]</code>,
together this gives <code>t(n+1) ≥ t(n)</code>.</p>

        <p>From this, we get:</p>

        <ul>
          <li><code>{d | d ∈ deletions, d ≥ t(n+1)} ⊆ {d | d ∈ deletions, d ≥ t(n)}</code></li>
          <li><code>deletions(t(n+1)) ⊆ deletions(t(n))</code></li>
          <li><code>|deletions(t(n+1))| ≤ |deletions(t(n))|</code></li>
          <li><code>|deletions| - |deletions(t(n+1))| ≥ |deletions| - |deletions(t(n))|</code></li>
          <li><code>offset(t(n+1)) ≥ offset(t(n))</code></li>
        </ul>

        <p>Together with lines 15-16 this gives us <code>off(n+1) ≥ off(n)</code>.</p>

        <p><strong>Corollary 1</strong>: The loop on lines 12-17 always terminates.<br />
<em>Proof</em>:<br />
Following the definitions, the end condition of the loop is <code>ori + off(n) = ori + off(n+1)</code>.
From Lemma 1 we know that <code>off</code> is a non-decreasing function.
Since <code>deletions</code> is a finite list of triples, there is an upper limit for <code>off</code> (<code>|deletions|</code>),
causing <code>off</code> to stop increasing at some point which triggers the end condition.</p>

        <p><strong>Corollary 2</strong>: When the loop on lines 12-17 terminates, <code>offset = |{d | d ∈ deletions, d ≤ snapshot[ori + offset]}|</code> and <code>ori + offset &lt; |snapshot|</code><br />
<em>Proof</em>:<br />
The first part follows from the definition of <code>deletions</code> and <code>offset</code>.
The second part follows from <code>offset ≤ |deletions|</code> and line 11.</p>

        <p><strong>Theorem 1</strong>: queryVm returns a sublist of <code>(snapshot\deletions + additions)</code>, starting at the given offset.<br />
<em>Proof</em>:<br />
If the given version is equal to a snapshot, there are no additions or deletions so this follows directly from lines 2-4.</p>

        <p>Following the definition of <code>deletions</code>, <code>∀ x ∈ deletions: x ∈ snapshot</code> and thus <code>|snapshot\deletions| = |snapshot| - |deletions|</code>.</p>

        <p>Due to the ordered nature of <code>snapshot</code> and <code>deletions</code>, if <code>ori &lt; |snapshot\deletions|</code>, version<code>[ori] = snapshot[ori + |D|]</code> with <code>D = {d | d ∈ deletions, d &lt; snapshot[ori + |D|]}</code>.
Due to <code>|snapshot\deletions| = |snapshot| - |deletions|</code>, this corresponds to the if-statement on line 11.
From Corollary 1 we know that the loop terminates
and from Corollary 2 and line 13 that snapshot points to the element at position
<code>ori + |{d | d ∈ deletions, d ≤ snapshot[ori + offset]}|</code> which,
together with <code>additions</code> starting at index 0 and line 25,
returns the requested result.</p>

        <p>If <code>ori ≥ |snapshot\deletions|</code>, <code>version[ori] = additions[ori - |snapshot\deletions|]</code>.
From lines 20-22 follows that <code>snapshot</code> gets emptied and <code>additions</code> gets shifted for the remaining required elements <code>(ori - |snapshot\deletions|)</code>, which then also returns the requested result on line 25.</p>

        <h3 id="delta-materialization">Delta Materialization</h3>

        <p>The goal of delta materialization (<span class='abbreviation' title='Delta materialization'>DM</span>) queries is to query the triple differences between two versions.
Furthermore, each triple in the result stream is annotated with either being an addition or deletion between the given version range.
Within the scope of this work, we limit ourselves to delta materialization within a single snapshot and delta chain.
Because of this, we distinguish between two different cases for our <span class='abbreviation' title='Delta materialization'>DM</span> algorithm
in which we can query triple patterns between a start and end version,
the start version of the query can either correspond to the snapshot version or it can come after that.
Furthermore, we introduce an equivalent algorithm for estimating the number of results for these queries.</p>

        <h4 id="query-1">Query</h4>

        <p>For the first query case, where the start version corresponds to the snapshot version,
the algorithm is straightforward.
Since we always store our deltas relative to the snapshot,
filtering the delta of the given end version based on the given triple pattern directly corresponds to the desired result stream.
Furthermore, we filter out local changes, as we are only interested in actual change with respect to the snapshot.</p>

        <p>For the second case, the start version does not correspond to the snapshot version.
The algorithm iterates over the triple pattern iteration scope of the addition and deletion trees in a sort-merge join-like operation,
and only emits the triples that have a different addition/deletion flag for the two versions.</p>

        <h4 id="estimated-count-1">Estimated count</h4>

        <p>For the first case, the start version corresponds to the snapshot version.
The estimated number of results is then the number of snapshot triples for the pattern summed up with the exact umber of deletions and additions for the pattern.</p>

        <p>In the second case the start version does not correspond to the snapshot version.
We estimate the total count as the sum of the additions and deletions for the given triple pattern in both versions.
This may only be a rough estimate, but will always be an upper bound, as the triples that were changed twice within the version range and negate each other
are also counted.
For exact counting, this number of negated triples should be subtracted.</p>

        <h3 id="version-query">Version Query</h3>

        <p>For version querying (<span class='abbreviation' title='Version query'>VQ</span>), the final query atom, we have to retrieve all triples across all versions,
annotated with the versions in which they exist.
In this work, we again focus on version queries for a single snapshot and delta chain.
For multiple snapshots and delta chains, the following algorithms can simply be applied once for each snapshot and delta chain.
In the following sections, we introduce an algorithm for performing triple pattern version queries
and an algorithm for estimating the total number of matching triples for the former queries.</p>

        <h4 id="query-2">Query</h4>

        <p>Our version querying algorithm is again based on a sort-merge join-like operation.
We start by iterating over the snapshot for the given triple pattern.
Each snapshot triple is queried within the deletion tree.
If such a deletion value can be found, the versions annotation contains all versions except for the versions
for which the given triple was deleted with respect to the given snapshot.
If no such deletion value was found, the triple was never deleted,
so the versions annotation simply contains all versions of the store.
Result stream offsetting can happen efficiently as long as the snapshot allows efficient offsets.
When the snapshot iterator is finished, we iterate over the addition tree in a similar way.
Each addition triple is again queried within the deletions tree
and the versions annotation can equivalently be derived.</p>

        <h4 id="estimated-count-2">Estimated count</h4>

        <p>Calculating the number of unique triples matching any triple pattern version query is trivial.
We simply retrieve the count for the given triple pattern in the given snapshot
and add the number of additions for the given triple pattern over all versions.
The number of deletions should not be taken into account here,
as this information is only required for determining the version annotation in the version query results.</p>

      </section>

  <section id="evaluation">
        <h2>Evaluation</h2>

        <p>In this section, we evaluate our proposed storage technique and querying algorithms.
We start by introducing <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, an implementation of our proposed solution.
After that, we describe the setup of our experiments, followed by presenting our results.
Finally, we discuss these results.</p>

        <h3 id="implementation">Implementation</h3>

        <p><span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> stands for <em>Offset-enabled STore for TRIple CHangesets</em>,
and it is a software implementation of the storage and querying techniques described in this article
It is implemented in C/C++ and available on <a href="https://zenodo.org/record/883008" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​883008">GitHub</a> under an open license.
In the scope of this work, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> currently supports a single snapshot and delta chain.
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> uses <a property="schema:citation http://purl.org/spar/cito/cites" href="http://www.websemanticsjournal.org/index.php/ps/article/view/328"><span class='abbreviation' title='Header Dictionary Triples'>HDT</span></a> <span class="references">[<a href="#ref-23">23</a>]</span> as snapshot technology as it conforms to all the <a href="#snapshot-storage">requirements</a> for our approach.
Furthermore, for our indexes we use <a href="http://fallabs.com/kyotocabinet/" class="mandatory" data-link-text="http:/​/​fallabs.com/​kyotocabinet/​">Kyoto Cabinet</a>,
which provides a highly efficient memory-mapped B+Tree implementation with compression support.
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> immediately generates the main <code>SPO</code> index and the auxiliary <code>OSP</code> and <code>POS</code> indexes.
In future work, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> could be modified to only generate the main index and delay auxiliary index generation to a later stage.
Memory-mapping is required so that not all data must be loaded in-memory when queries are evaluated,
which would not always be possible for large datasets.
For our delta dictionary, we extend <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>’s dictionary implementation with adjustments to make it work with unsorted triple components.
We compress this delta dictionary with <a href="http://www.gzip.org/">gzip</a>, which requires decompression during querying and ingestion.
Finally, for storing our addition counts, we use the Hash Database of Kyoto Cabinet, which is also memory-mapped.</p>

        <p>We provide a developer-friendly C/C++ API for ingesting and querying data based on an <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> store.
Additionally, we provide command-line tools for ingesting data into an <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> store,
or evaluating <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> or <span class='abbreviation' title='Version query'>VQ</span> triple pattern queries for any given limit and offset against a store.
Furthermore, we implemented <a href="https://zenodo.org/record/883010" class="mandatory" data-link-text="https:/​/​zenodo.org/​record/​883010">Node JavaScript bindings</a> that
expose the <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> API for ingesting and querying to JavaScript applications.
We used these bindings to <a href="http://versioned.linkeddatafragments.org/bear" class="mandatory" data-link-text="http:/​/​versioned.linkeddatafragments.org/​bear">expose an <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> store</a>
containing a dataset with 30M triples in 10 versions using <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/doi:10.1016/j.websem.2016.03.003"><a href="http://linkeddatafragments.org/publications/jws2016.pdf"><span class='abbreviation' title='Triple Pattern Fragments'>TPF</span></a></span> <span class="references">[<a href="#ref-8">8</a>]</span>, with the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><span class='abbreviation' title='Versioned Triple Pattern Fragments'>VTPF</span> feature</a> <span class="references">[<a href="#ref-10">10</a>]</span>.</p>

        <h3 id="experimental-setup">Experimental Setup</h3>

        <p>As mentioned before in <a href="#related-work-benchmarks">Subsection 2.3</a>, we evaluate our approach using the <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> benchmark.
We chose for this benchmark because it provides a complete set of tools and data for benchmarking <span class='abbreviation' title='Resource Description Framework'>RDF</span> versioning systems,
containing datasets, queries and easy-to-use engines to compare with.</p>

        <p>We extended the existing <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> implementation for the evaluation of offsets.
We did this by implementing custom offset features into each of the <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> approaches.
Only for <span class='abbreviation' title='Version materialization'>VM</span> queries in <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> an efficient implementation (<span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>+) could be made because of <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>’s native offset capabilities.
In all other cases, naive offsets had to be implemented by iterating over the result stream
until a number of elements equal to the desired offset were consumed.
This modified implementation is available on <a href="https://github.com/rdfostrich/bear/tree/ostrich-eval-journal" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​bear/​tree/​ostrich-​eval-​journal">GitHub</a>.
To test the scalability of our approach for datasets with few and large versions, we use the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> benchmark.
We use the ten first versions of the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> dataset, which contains 30M to 66M triples per version.
This dataset was compiled from the <a href="http://swse.deri.org/dyldo/">Dynamic Linked Data Observatory</a>.
To test for datasets with many smaller versions, we use <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> with the daily and hourly granularities.
The daily dataset contains 89 versions and the hourly dataset contains 1,299 versions,
both of them have around 48K triples per version.
We did not evaluate <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-instant, because <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires increasingly
more time for each new version ingestion, as will be shown in the next section.
As <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly with 1,299 versions already takes more than three days to ingest,
the 21,046 versions from <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-instant would require too much time to ingest.
Our experiments were executed on a 64-bit
Ubuntu 14.04 machine with 128 GB of memory and a
24-core 2.40 GHz CPU.</p>

        <p>For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, we use all 7 of the provided querysets, each containing at most 50 triple pattern queries,
once with a high result cardinality and once with a low result cardinality.
These querysets correspond to all possible triple pattern materializations, except for triple patterns where each component is blank.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>, only two querysets are provided, those that correspond to <code>?P?</code> and <code>?PO</code> queries.
The number of <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> queries is more limited, but they are derived from real-world DBpedia queries
which makes them useful for testing real-world applicability.
All of these queries are evaluated as <span class='abbreviation' title='Version materialization'>VM</span> queries on all versions,
as <span class='abbreviation' title='Delta materialization'>DM</span> between the first version and all other versions,
and as <span class='abbreviation' title='Version query'>VQ</span>.</p>

        <p>For a complete comparison with other approaches, we re-evaluated <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>’s Jena and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive implementations.
More specifically, we ran all <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> queries against Jena with the <span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Change-based'>CB</span>, <span class='abbreviation' title='Timestamp-based'>TB</span> and hybrid <span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> implementation,
and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> with the <span class='abbreviation' title='Independent copies'>IC</span> and <span class='abbreviation' title='Change-based'>CB</span> implementations
using the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> dataset for ten versions.
We did the same for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> with the daily and hourly dataset.
After that, we evaluated <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> for the same queries and datasets.
We were not able to extend this benchmark with other similar systems such as <span class='abbreviation' title=''>X-<span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span></span>, <span class='abbreviation' title='Resource Description Framework'>RDF</span>-TX and Dydra,
because the source code of systems was either not publicly available,
or the system would require additional implementation work to support the required query interfaces.</p>

        <p>Additionally, we evaluated the ingestion rates and storage sizes for all approaches.
Furthermore, we compared the ingestion rate for the two different ingestion algorithms of <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>.
The batch-based algorithm expectedly ran out of memory for larger amounts of versions,
so we used the streaming-based algorithm for all further evaluations.</p>

        <p>Finally, we evaluated the offset capabilities of <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>
by comparing it with custom offset implementations for the other approaches.
We evaluated the blank triple pattern query with offsets ranging from 2 to 4,096 with a limit of 10 results.</p>

        <h3 id="results">Results</h3>

        <p>In this section, we present the results of our evaluation.
We report the ingestion results, compressibility, query evaluation times for all cases and offset result.
All raw results and the scripts that were used to process them are available on <a href="https://github.com/rdfostrich/ostrich-bear-results/" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​ostrich-​bear-​results/​">GitHub</a>.</p>

        <h4 id="ingestion">Ingestion</h4>

        <p><a href="#results-ingestion-size">Table 7</a> and <a href="#results-ingestion-time">Table 8</a>
respectively show the storage requirements and ingestion times for the different approaches for the three different benchmarks.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, the <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches outperform <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> in terms of ingestion time, they are about two orders of magniture faster.
Only <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> requires slightly less storage space.
The Jena-based approaches ingest one order of magnitude faster than <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, but require more storage space.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires less storage space than all other approaches except for <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> at the cost of slower ingestion.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly, only <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> and Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> require about 8 to 4 times less space than <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> even requires less storage space than gzip on raw N-Triples.</p>

        <p>As mentioned in <a href="#addition-counts">Subsection 5.4</a>, we use a threshold to define which addition count values should be stored,
and which ones should be evaluated at query time.
For our experiments, we fixed this count threshold at 200,
which has been empirically determined through various experiments as a good value.
For values higher than 200, the addition counts started having a noticable impact on the performance of count estimation.
This threshold value means that when a triple pattern has 200 matching additions,
then this count will be stored.
<a href="#results-addition-counts">Table 9</a> shows that the storage space of the addition count datastructure
in the case of <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly is insignificant compared to the total space requirements.
However, for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily, addition counts take up 37.05% of the total size with still an acceptable absolute size,
as the addition and deletion trees require relatively less space,
because of the lower amount of versions.
Within the scope of this work, we use this fixed threshold of 200.
We consider investigating the impact of different threshold levels and methods for dynamically determining optimal levels future work.</p>

        <p><a href="#results-ostrich-ingestion-rate-beara">Fig. 5</a> shows linearly increasing ingestion rate for each consecutive version for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>,
while <a href="#results-ostrich-ingestion-size-beara">Fig. 6</a> shows corresponding linearly increasing storage sizes.
Analogously, <a href="#results-ostrich-ingestion-rate-bearb-hourly">Fig. 7</a> shows the ingestion rate for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly,
which increases linearly until around version 1100, after which it increases significantly.
<a href="#results-ostrich-ingestion-size-bearb-hourly">Fig. 8</a> shows near-linearly increasing storage sizes.</p>

        <p><a href="#results-ostrich-ingestion-rate-beara-compare">Fig. 9</a> compares the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> ingestion rate of the streaming and batch algorithms.
The streaming algorithm starts of slower than the batch algorithm but grows linearly,
while the batch algorithm consumes a large amount of memory, resulting in slower ingestion after version 8 and an out-of-memory error after version 10.</p>

        <figure id="results-ingestion-size" class="table">

          <table>
            <thead>
              <tr>
                <th>Approach</th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Raw (N-Triples)</td>
                <td style="text-align: right">46,069.76</td>
                <td style="text-align: right">556.44</td>
                <td style="text-align: right">8,314.86</td>
              </tr>
              <tr>
                <td>Raw (gzip)</td>
                <td style="text-align: right">3,194.88</td>
                <td style="text-align: right">30.98</td>
                <td style="text-align: right">466.35</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span></td>
                <td style="text-align: right">3,102.72</td>
                <td style="text-align: right">12.32</td>
                <td style="text-align: right">187.46</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">+1,484.80</td>
                <td style="text-align: right">+4.55</td>
                <td style="text-align: right">+263.13</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Independent copies'>IC</span></td>
                <td style="text-align: right">32,808.96</td>
                <td style="text-align: right">415.32</td>
                <td style="text-align: right">6,233.92</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Change-based'>CB</span></td>
                <td style="text-align: right">18,216.96</td>
                <td style="text-align: right">42.82</td>
                <td style="text-align: right">473.41</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Timestamp-based'>TB</span></td>
                <td style="text-align: right">82,278.4</td>
                <td style="text-align: right">23.61</td>
                <td style="text-align: right">3,678.89</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span></td>
                <td style="text-align: right">31,160.32</td>
                <td style="text-align: right">22.83</td>
                <td style="text-align: right">53.84</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span></td>
                <td style="text-align: right">5,335.04</td>
                <td style="text-align: right">142.08</td>
                <td style="text-align: right">2,127.57</td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">+1,494.69</td>
                <td style="text-align: right">+6.53</td>
                <td style="text-align: right">+98.88</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span></td>
                <td style="text-align: right"><em>2,682.88</em></td>
                <td style="text-align: right"><em>5.96</em></td>
                <td style="text-align: right"><em>24.39</em></td>
              </tr>
              <tr>
                <td> </td>
                <td style="text-align: right">+802.55</td>
                <td style="text-align: right">+0.25</td>
                <td style="text-align: right">+0.75</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 7:</span> Storage sizes for each of the <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive approaches in MB with <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
The additional storage size for the auxiliary <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> indexes are provided as separate rows.
The lowest sizes per dataset are indicated in italics.</p>
          </figcaption>
        </figure>

        <figure id="results-ingestion-time" class="table">

          <table>
            <thead>
              <tr>
                <th>Approach</th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span></td>
                <td style="text-align: right">2,256</td>
                <td style="text-align: right">12.36</td>
                <td style="text-align: right">4,497.32</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Independent copies'>IC</span></td>
                <td style="text-align: right">443</td>
                <td style="text-align: right">8.91</td>
                <td style="text-align: right">142.26</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Change-based'>CB</span></td>
                <td style="text-align: right">226</td>
                <td style="text-align: right">9.53</td>
                <td style="text-align: right">173.48</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Timestamp-based'>TB</span></td>
                <td style="text-align: right">1,746</td>
                <td style="text-align: right">0.35</td>
                <td style="text-align: right">70.56</td>
              </tr>
              <tr>
                <td>Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span></td>
                <td style="text-align: right">679</td>
                <td style="text-align: right">0.35</td>
                <td style="text-align: right">0.65</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span></td>
                <td style="text-align: right">34</td>
                <td style="text-align: right">0.39</td>
                <td style="text-align: right">5.89</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span></td>
                <td style="text-align: right"><em>18</em></td>
                <td style="text-align: right"><em>0.02</em></td>
                <td style="text-align: right"><em>0.07</em></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 8:</span> Ingestion times for each of the <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive approaches with <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
The lowest times per dataset are indicated in italics.</p>
          </figcaption>
        </figure>

        <figure id="results-addition-counts" class="table">

          <table>
            <thead>
              <tr>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</th>
                <th style="text-align: right"><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align: right">13.69 (0.29%)</td>
                <td style="text-align: right">6.25 (37.05%)</td>
                <td style="text-align: right">15.62 (3.46%)</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 9:</span> Storage sizes of the <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> addition count component in MB with <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
The percentage of storage space that this component requires compared to the complete store is indicated between brackets.</p>
          </figcaption>
        </figure>

        <figure id="results-ostrich-ingestion-rate-beara">
<img src="img/results-ostrich-ingestion-rate-beara.svg" alt="[bear-a ostrich ingestion rate]" height="150em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 5:</span> <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> ingestion durations for each consecutive <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> version in minutes for an increasing number of versions,
showing a lineair growth.</p>
          </figcaption>
</figure>

        <figure id="results-ostrich-ingestion-size-beara">
<img src="img/results-ostrich-ingestion-size-beara.svg" alt="[bear-a ostrich ingestion sizes]" height="150em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 6:</span> Cumulative <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> store sizes for each consecutive <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> version in GB for an increasing number of versions,
showing a lineair growth.</p>
          </figcaption>
</figure>

        <figure id="results-ostrich-ingestion-rate-bearb-hourly">
<img src="img/results-ostrich-ingestion-rate-bearb-hourly.svg" alt="[bear-b-hourly ostrich ingestion rate]" height="150em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 7:</span> <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> ingestion durations for each consecutive <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly version in minutes for an increasing number of versions.</p>
          </figcaption>
</figure>

        <figure id="results-ostrich-ingestion-size-bearb-hourly">
<img src="img/results-ostrich-ingestion-size-bearb-hourly.svg" alt="[bear-b-hourly ostrich ingestion sizes]" height="150em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 8:</span> Cumulative <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> store sizes for each consecutive <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly version in GB for an increasing number of versions.</p>
          </figcaption>
</figure>

        <figure id="results-ostrich-compressability" class="table">

          <table>
            <thead>
              <tr>
                <th>Format</th>
                <th>Dataset</th>
                <th style="text-align: right">Size</th>
                <th style="text-align: right">gzip</th>
                <th style="text-align: right">Savings</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>N-Triples</strong></td>
                <td>A</td>
                <td style="text-align: right">46,069.76</td>
                <td style="text-align: right">3,194.88</td>
                <td style="text-align: right">93.07%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right">8,314.86</td>
                <td style="text-align: right">466.35</td>
                <td style="text-align: right">94.39%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right">556.44</td>
                <td style="text-align: right">30.98</td>
                <td style="text-align: right">94.43%</td>
              </tr>
              <tr>
                <td><strong><span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span></strong></td>
                <td>A</td>
                <td style="text-align: right">3,117.64</td>
                <td style="text-align: right">2,155.13</td>
                <td style="text-align: right">95.32%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right">187.46</td>
                <td style="text-align: right">34.92</td>
                <td style="text-align: right">99.58%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right">12.32</td>
                <td style="text-align: right">3.35</td>
                <td style="text-align: right">99.39%</td>
              </tr>
              <tr>
                <td><strong><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span></strong></td>
                <td>A</td>
                <td style="text-align: right">5,335.04</td>
                <td style="text-align: right">1,854.48</td>
                <td style="text-align: right">95.97%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right">2,127.57</td>
                <td style="text-align: right">388.02</td>
                <td style="text-align: right">95.33%</td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right">142.08</td>
                <td style="text-align: right">25.69</td>
                <td style="text-align: right">95.33%</td>
              </tr>
              <tr>
                <td><strong><span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span></strong></td>
                <td>A</td>
                <td style="text-align: right"><em>2,682.88</em></td>
                <td style="text-align: right"><em>856.39</em></td>
                <td style="text-align: right"><em>98.14%</em></td>
              </tr>
              <tr>
                <td> </td>
                <td>B-hourly</td>
                <td style="text-align: right"><em>24.39</em></td>
                <td style="text-align: right"><em>2.86</em></td>
                <td style="text-align: right"><em>99.96%</em></td>
              </tr>
              <tr>
                <td> </td>
                <td>B-daily</td>
                <td style="text-align: right"><em>5.96</em></td>
                <td style="text-align: right"><em>1.14</em></td>
                <td style="text-align: right"><em>99.79%</em></td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 10:</span> Compressability using gzip for all <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> datasets using <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> and natively as N-Triples.
The columns represent the original size (MB), the resulting size after applying gzip (MB), and the total space savings.
The lowest sizes are indicated in italics.</p>
          </figcaption>
        </figure>

        <figure id="results-ostrich-ingestion-rate-beara-compare">
<img src="img/results-ostrich-ingestion-rate-beara-compare.svg" alt="[Comparison of ostrich ingestion algorithms]" height="150em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 9:</span> Comparison of the <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> stream and batch-based ingestion durations.</p>
          </figcaption>
</figure>

        <h4 id="compressibility">Compressibility</h4>

        <p><a href="#results-ostrich-compressability">Table 10</a> presents the compressibility of datasets without auxiliary indexes,
showing that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and the <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches significantly improve compressibility compared to the original N-Triples serialization.
We omitted the results from the Jena-based approaches in this table,
as all compressed sizes were in all cases two to three times larger than the N-Triples compression.</p>

        <h4 id="query-evaluation">Query Evaluation</h4>

        <p>Figures <a href="#results-beara-vm-sumary">10</a>, <a href="#results-beara-dm-summary">11</a> and <a href="#results-beara-vq-summary">12</a> respectively
summarize the <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> query durations of all <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> queries on the ten first versions of the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> dataset for the different approaches.
<span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> clearly outperforms all other approaches in all cases,
while the Jena-based approaches are orders of magnitude slower than the <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches and <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> in all cases.
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is about two times faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> for <span class='abbreviation' title='Version materialization'>VM</span> queries, and slightly slower for both <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries.
For <span class='abbreviation' title='Delta materialization'>DM</span> queries, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> does however continuously become slower for larger versions, while the lookup times for <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> remain constant.
From version 7, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>.
<a href="#appendix-bear-a">Appendix A</a> contains more detailed plots for each <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> queryset,
in which we can see that all approaches collectively become slower for queries with a higher result cardinality,
and that predicate-queries are also significantly slower for all approaches.</p>

        <figure id="results-beara-vm-sumary">
<img src="img/query/results_beara-vm-summary.svg" alt="[bear-a vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 10:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> <span class='abbreviation' title='Version materialization'>VM</span> query results for all triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="results-beara-dm-summary">
<img src="img/query/results_beara-dm-summary.svg" alt="[bear-a dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 11:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> <span class='abbreviation' title='Delta materialization'>DM</span> query results for all triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="results-beara-vq-summary">
<img src="img/query/results_beara-vq-summary.svg" alt="[bear-a vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 12:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> <span class='abbreviation' title='Version query'>VQ</span> query results for all triple patterns.</p>
          </figcaption>
</figure>

        <p>Figures <a href="#results-bearb-daily-vm-sumary">13</a>, <a href="#results-bearb-daily-dm-summary">14</a> and <a href="#results-bearb-daily-vq-summary">15</a>
contain the query duration results for the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> queries on the complete <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily dataset for the different approaches.
Jena-based approaches are again slower than both the <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based ones and <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>.
For <span class='abbreviation' title='Version materialization'>VM</span> queries, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is slower than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, but faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, which becomes slower for larger versions.
For <span class='abbreviation' title='Delta materialization'>DM</span> queries, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> for the second half of the versions, and slightly faster <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>.
The difference between <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> and <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is however insignificant in this case, as can be seen in <a href="#appendix-bear-b-daily">Appendix B</a>.
For <span class='abbreviation' title='Version query'>VQ</span> queries, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is significantly faster than all other approaches.
<a href="#appendix-bear-b-daily">Appendix B</a> contains more detailed plots for this case,
in which we can see that predicate-queries are again consistently slower for all approaches.</p>

        <figure id="results-bearb-daily-vm-sumary">
<img src="img/query/results_bearb-daily-vm-summary.svg" alt="[bear-b-daily vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 13:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily <span class='abbreviation' title='Version materialization'>VM</span> query results for all triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="results-bearb-daily-dm-summary">
<img src="img/query/results_bearb-daily-dm-summary.svg" alt="[bear-b-daily dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 14:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily <span class='abbreviation' title='Delta materialization'>DM</span> query results for all triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="results-bearb-daily-vq-summary">
<img src="img/query/results_bearb-daily-vq-summary.svg" alt="[bear-b-daily vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 15:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily <span class='abbreviation' title='Version query'>VQ</span> query results for all triple patterns.</p>
          </figcaption>
</figure>

        <p>Figures <a href="#results-bearb-hourly-vm-sumary">16</a>, <a href="#results-bearb-hourly-dm-summary">17</a> and <a href="#results-hourly-daily-vq-summary">18</a>
show the query duration results for the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span> queries on the complete <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly dataset for all approaches.
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> again outperforms Jena-based approaches in all cases.
<span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> is faster for <span class='abbreviation' title='Version materialization'>VM</span> queries than <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, but <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> is significantly slower, except for the first 100 versions.
For <span class='abbreviation' title='Delta materialization'>DM</span> queries, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is comparable to <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, and faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, except for the first 100 versions.
Finally, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> outperforms all <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches for <span class='abbreviation' title='Version query'>VQ</span> queries by almost an order of magnitude.
<a href="#appendix-bear-b-hourly">Appendix C</a> contains the more detailed plots
with the same conclusion as before that predicate-queries are slower.</p>

        <figure id="results-bearb-hourly-vm-sumary">
<img src="img/query/results_bearb-hourly-vm-summary.svg" alt="[bear-b-hourly vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 16:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly <span class='abbreviation' title='Version materialization'>VM</span> query results for all triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="results-bearb-hourly-dm-summary">
<img src="img/query/results_bearb-hourly-dm-summary.svg" alt="[bear-b-hourly dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 17:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly <span class='abbreviation' title='Delta materialization'>DM</span> query results for all triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="results-bearb-hourly-vq-summary">
<img src="img/query/results_bearb-hourly-vq-summary.svg" alt="[bear-b-hourly vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 18:</span> Median <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly <span class='abbreviation' title='Version query'>VQ</span> query results for all triple patterns.</p>
          </figcaption>
</figure>

        <h4 id="offset">Offset</h4>

        <p>From our evaluation of offsets, <a href="#results-offset-vm">Fig. 19</a> shows that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> offset evaluation remain below 1ms,
while other approaches grow beyond that for larger offsets, except for <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>+.
<span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Change-based'>CB</span> and Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> are not included in this and the following figures
because they require full materialization before offsets can be applied, which is expensive and therefore take a very long time to evaluate.
For <span class='abbreviation' title='Delta materialization'>DM</span> queries, all approaches have growing evaluating times for larger offsets including <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, as can be seen in <a href="#results-offset-dm">Fig. 20</a>.
Finally, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> has <span class='abbreviation' title='Version query'>VQ</span> evaluation times that are approximately independent of the offset value,
while other approaches again have growing evaluation times, as shown in <a href="#results-offset-vq">Fig. 21</a>.</p>

        <figure id="results-offset-vm">
<img src="img/query/results_offsets-vm.svg" alt="[Offsets vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 19:</span> Median <span class='abbreviation' title='Version materialization'>VM</span> query results for different offsets over all versions in the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> dataset.</p>
          </figcaption>
</figure>

        <figure id="results-offset-dm">
<img src="img/query/results_offsets-dm.svg" alt="[Offsets dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 20:</span> Median <span class='abbreviation' title='Delta materialization'>DM</span> query results for different offsets between version 0 and all other versions in the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> dataset.</p>
          </figcaption>
</figure>

        <figure id="results-offset-vq">
<img src="img/query/results_offsets-vq.svg" alt="[Offsets vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 21:</span> Median <span class='abbreviation' title='Version query'>VQ</span> query results for different offsets in the <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> dataset.</p>
          </figcaption>
</figure>

        <h3 id="discussion">Discussion</h3>

        <p>In this section, we interpret and discuss the results from previous section.
We discuss the ingestion, compressbility, query evaluation, offset efficiency and test our hypotheses.</p>

        <h4 id="ingestion-1">Ingestion</h4>

        <p>For all evaluated cases, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires less storage space than most non-<span class='abbreviation' title='Change-based'>CB</span> approaches.
The <span class='abbreviation' title='Change-based'>CB</span> and <span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> approaches in most cases outperform <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> in terms of storage space efficiency due
to the additional metadata that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> stores per triple.
Because of this, most other approaches require less time to ingest new data.
These timing results should however be interpreted correctly,
because all other approaches receive their input data in the appropriate format (<span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Change-based'>CB</span>, <span class='abbreviation' title='Timestamp-based'>TB</span>, <span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span>),
while <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> does not.
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> must convert <span class='abbreviation' title='Change-based'>CB</span> input at runtime to the alternative <span class='abbreviation' title='Change-based'>CB</span> structure where deltas are relative to the snapshot,
which explains the larger ingestion times.
As an example, <a href="#triples-bearb-hourly-altcb">Fig. 22</a> shows the number of triples in each <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly version
where the deltas have been transformed to the alternative delta structure that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> uses.
Just like the first part of <a href="#results-ostrich-ingestion-rate-bearb-hourly">Fig. 7</a>, this graph also increases linearly,
which indicates that the large number of triples that need to be handled for long delta chains is one of the main bottlenecks for <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>.
This is also the reason why <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> has memory issues during ingestion at the end of such chains.
One future optimization could be to maintain the last version of each chain in a separate index for faster patching.
Or a new ingestion algorithm could be implemented that accepts input in the correct alternative <span class='abbreviation' title='Change-based'>CB</span> format.
Alternatively, a new snapshot could dynamically be created when ingestion time becomes too large,
which could for example for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly take place around version 1000.</p>

        <figure id="triples-bearb-hourly-altcb">
<img src="img/triples-bearb-hourly-altcb.svg" alt="[bear-b-hourly alternative cb]" height="150em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 22:</span> Total number of triples for each <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly version when converted to the alternative <span class='abbreviation' title='Change-based'>CB</span> structure used by <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>,
i.e., each triple is an addition or deletion relative to the <em>first</em> version instead of the <em>previous</em> version.</p>
          </figcaption>
</figure>

        <p>The <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly datasets indicate the limitations of the ingestion algorithm in our system.
The results for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> show that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> ingests slowly for many very large versions,
but it is still possible because of the memory-efficient streaming algorithm.
The results for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly show that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> should not be used when the number of versions is very large.
Furthermore, for each additional version in a dataset, the ingestion time increases.
This is a direct consequence of our alternative delta chain method where all deltas are relative to a snapshot.
That is the reason why when new deltas are inserted,
the previous one must be fully materialized by iterating over all existing triples,
because no version index exists.</p>

        <p>In <a href="#results-ostrich-ingestion-rate-bearb-hourly">Fig. 7</a>, we can observe large fluctuations in ingestion time around version 1,200 of <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
This is caused by the large amount of versions that are stored for each tree value.
Since each version requires a mapping to seven triple pattern indexes and one local change flag in the deletion tree,
value sizes become non-negligible for large amounts of versions.
Each version value requires 28 uncompressed bytes,
which results in more than 32KB for a triple in 1,200 versions.
At that point, the values start to form a bottleneck as only 1,024 elements
can be loaded in-memory using the default page cache size of 32MB,
which causes a large amount of swapping.
This could be solved by either tweaking the B+Tree parameters for this large amount of versions,
reducing storage requirements for each value,
or by dynamically creating a new snapshot.</p>

        <p>We compared the streaming and batch-based ingestion algorithm in <a href="#results-ostrich-ingestion-rate-beara-compare">Fig. 9</a>.
The batch algorithm is initially faster because most operations can happen in memory,
while the streaming algorithm only uses a small fraction of that memory,
which makes the latter usable for very large datasets that don’t fit in memory.
In future work, a hybrid between the current streaming and batch algorithm could be investigated,
i.e., a streaming algorithm with a larger buffer size, which is faster, but doesn’t require unbounded amounts of memory.</p>

        <h4 id="compressibility-1">Compressibility</h4>

        <p>As shown in <a href="#results-ostrich-compressability">Table 10</a>,
when applying gzip directly on the raw N-Triples input already achieves significant space savings.
However, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> are able to reduce the required storage space <em>even further</em> when they are used as a pre-processing step before applying gzip.
This shows that these approaches are better—storage-wise—for the archival of versioned datasets.
This table also shows that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> datasets with more versions are more prone to space savings
using compression techniques like gzip compared to <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> datasets with fewer versions.</p>

        <h4 id="query-evaluation-1">Query Evaluation</h4>

        <p>The results from previous section show that the <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> query evaluation efficiency is faster than all Jena-based approaches,
mostly faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, and mostly slower than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>.
<span class='abbreviation' title='Version materialization'>VM</span> queries in <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> are always slower than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>,
because <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> can very efficiently query a single materialized snapshot in this case,
while <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires more operations for materializing.
<span class='abbreviation' title='Version materialization'>VM</span> queries in <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> are however always faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, because the latter has to reconstruct complete delta chains,
while <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> only has to reconstruct a single delta relative to the snapshot.
For <span class='abbreviation' title='Delta materialization'>DM</span> queries, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is slower or comparable to <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, slower than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> for early versions, but faster for later versions.
This slowing down of <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> for <span class='abbreviation' title='Delta materialization'>DM</span> queries is again caused by reconstruction of delta chains.
For <span class='abbreviation' title='Version query'>VQ</span> queries, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> outperforms all other approaches for datasets with larger amounts of versions.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, which contains only 10 versions in our case,
the <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches are slightly faster because only a small amount of versions need to be iterated.</p>

        <h4 id="offsets">Offsets</h4>

        <p>One of our initial requirements was to design a system that allows efficient offsetting of <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> result streams.
As shown in last section, for both <span class='abbreviation' title='Version materialization'>VM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries, the lookup times for various offsets remain approximately constant.
For <span class='abbreviation' title='Version materialization'>VM</span> queries, this can fluctuate slightly for certain offsets due to the loop section inside the <span class='abbreviation' title='Version materialization'>VM</span> algorithm
for determining the starting position inside the snapshot and deletion tree.
For <span class='abbreviation' title='Delta materialization'>DM</span> queries, we do however observe an increase in lookup times for larger offsets.
That is because the current <span class='abbreviation' title='Delta materialization'>DM</span> algorithm naively offsets these streams by iterating
over the stream until a number of elements equal to the desired offset have been consumed.
Furthermore, other <span class='abbreviation' title='Independent copies'>IC</span> and <span class='abbreviation' title='Timestamp-based'>TB</span> approaches outperform <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>’s <span class='abbreviation' title='Delta materialization'>DM</span> result stream offsetting.
This introduces a new point of improvement for future work,
seeing whether or not <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> would allow more efficient <span class='abbreviation' title='Delta materialization'>DM</span> offsets by adjusting either the algorithm or the storage format.</p>

        <h4 id="hypotheses">Hypotheses</h4>

        <p>In <a href="#problem-statement">Section 3</a>, we introduced six hypotheses, which we will validate in this section based on our experimental results.
We will only consider the comparison between <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-based approaches,
as <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> outperforms the Jena-based approaches for all cases in terms of lookup times.
These validations were done using R, for which the source code can be found on <a href="https://github.com/rdfostrich/ostrich-bear-results/" class="mandatory" data-link-text="https:/​/​github.com/​rdfostrich/​ostrich-​bear-​results/​">GitHub</a>.
Tables containing p-values of the results can be found in <a href="#appendix-tests">Appendix E</a>.</p>

        <p>For our <a href="#hypothesis-qualitative-querying">first hypothesis</a>, we expect <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> lookup times to remain independent of version for <span class='abbreviation' title='Version materialization'>VM</span> and <span class='abbreviation' title='Delta materialization'>DM</span> queries.
We validate this hypothesis by building a linear regression model with as response the lookup time,
and as factors version and number of results.
<a href="#hypo-test-1">Table 11</a> in the appendix contains the influence of each factor, which shows that for all cases,
we can accept the null hypothesis that the version factor has no influence on the models with a confidence of 99%.
Based on these results, we <em>accept</em> our <a href="#hypothesis-qualitative-querying">first hypothesis</a>.</p>

        <p><a href="#hypothesis-qualitative-ic-storage">Hypothesis 2</a> states that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires <em>less</em> storage space than <span class='abbreviation' title='Independent copies'>IC</span>-based approaches,
and <a href="#hypothesis-qualitative-ic-querying">Hypothesis 3</a> correspondingly states that
query evaluation is <em>slower</em> for <span class='abbreviation' title='Version materialization'>VM</span> and <em>faster</em> or <em>equal</em> for <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span>.
Results from previous section showed that for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly,
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires <em>less</em> space than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, which means that we <em>accept</em> Hypothesis 2.
In order to validate that query evaluation is slower for <span class='abbreviation' title='Version materialization'>VM</span> but faster or equal for <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span>,
we compared the means using the two-sample t-test, for which the results can be found in <a href="#hypo-test-2">Table 12</a> in the appendix.
In all cases, the means are not equal with a confidence of 95%.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> is faster for <span class='abbreviation' title='Version materialization'>VM</span> queries, but slower for <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries.
For <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> is faster for all query types.
We therefore <em>reject</em> Hypothesis 3, as it does not apply for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, but it is valid for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
This means that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> typically requires less storage space than <span class='abbreviation' title='Independent copies'>IC</span>-based approaches,
and outperforms other approaches in terms of querying efficiency
unless the number of versions is small or for <span class='abbreviation' title='Version materialization'>VM</span> queries.</p>

        <p>In <a href="#hypothesis-qualitative-cb-storage">Hypothesis 4</a>, we stated that <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires <em>more</em>
storage space than <span class='abbreviation' title='Change-based'>CB</span>-based approaches,
and in <a href="#hypothesis-qualitative-cb-querying">Hypothesis 5</a> that query evaluation is <em>faster</em> or <em>equal</em>.
In all cases <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires more storage space than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, which is why we <em>accept</em> Hypothesis 4.
For the query evaluation, we again compare the means in <a href="#hypo-test-3">Table 13</a> in the appendix using the same test.
In <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title='Version query'>VQ</span> queries in <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> are not faster for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, and <span class='abbreviation' title='Version materialization'>VM</span> queries in <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> are not faster for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily,
which is why we <em>reject</em> Hypothesis 5.
However, only one in three query atoms are not fulfilled, and <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is faster than <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
In general, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> requires more storage space than <span class='abbreviation' title='Change-based'>CB</span>-based approaches,
and query evaluation is faster unless the number of versions is low.</p>

        <p>Finally, in our <a href="#hypothesis-qualitative-ingestion">last hypothesis</a>,
we state that average query evaluation times are lower than other non-<span class='abbreviation' title='Independent copies'>IC</span> approaches at the cost of increased ingestion times.
In all cases, the ingestion time for <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is higher than the other approaches,
and as shown in <a href="#hypo-test-3">Table 13</a> in the appendix, query evaluation times for non-<span class='abbreviation' title='Independent copies'>IC</span> approaches are lower for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
This means that we <em>reject</em> Hypothesis 6 because it only holds for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly and not for <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily.
In general, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> ingestion is slower than other approaches,
but improves query evaluation time compared to other non-<span class='abbreviation' title='Independent copies'>IC</span> approaches,
unless the number of versions is low.</p>

        <p>In this section, we accepted three of the six hypotheses.
As these are statistical hypotheses, these do not necessarily indicate negative results of our approach.
Instead, they allow us to provide general guidelines on where our approach can be used effectively, and where not.</p>

      </section>

  <section id="conclusions">
        <h2>Conclusions</h2>

        <p>In this article, we introduced an <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive storage method with accompanied algorithms for evaluating <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span>, and <span class='abbreviation' title='Version query'>VQ</span> queries,
with efficient result offsets.
Our novel storage technique is a hybrid of the <span class='abbreviation' title='Independent copies'>IC</span>/<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span> approaches, because we store sequences of snapshots followed by delta chains.
The evaluation of our <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> implementation shows that this technique offers a new trade-off in terms of ingestion time, storage size and lookup times.
By preprocessing and storing additional data during ingestion, we can reduce lookup times for <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries compared to <span class='abbreviation' title='Change-based'>CB</span> and <span class='abbreviation' title='Timestamp-based'>TB</span> approaches.
Our approach requires less storage space than <span class='abbreviation' title='Independent copies'>IC</span> approaches, at the cost of slightly slower <span class='abbreviation' title='Version materialization'>VM</span> queries, but comparable <span class='abbreviation' title='Delta materialization'>DM</span> queries.
Furthermore, our technique is faster than <span class='abbreviation' title='Change-based'>CB</span> approaches, at the cost of more storage space.
Additionally, <span class='abbreviation' title='Version query'>VQ</span> queries become increasingly more efficient for datasets with larger amounts of versions compared to <span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Change-based'>CB</span> and <span class='abbreviation' title='Timestamp-based'>TB</span> approaches.
Our current implementation supports a single snapshot and delta chain
as a proof of concept,
but production environments would normally incorporate more frequent snapshots,
balancing between storage and querying requirements.</p>

        <p>With lookup times of 1ms or less in most cases, <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is an ideal candidate for Web querying,
as the network latency will typically be higher than that.
At the cost of increased ingestion times, lookups are fast.
Furthermore, by reusing the highly efficient <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> format for snapshots,
existing <span class='abbreviation' title='Header Dictionary Triples'>HDT</span> files can directly be loaded by <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>
and patched with additional versions afterwards.</p>

        <p><span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> fulfills the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf">requirements</a> <span class="references">[<a href="#ref-51">51</a>]</span> for a backend <span class='abbreviation' title='Resource Description Framework'>RDF</span> archive storage solution
for supporting versioning queries in the <span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> framework.
Together with the <a property="schema:citation http://purl.org/spar/cito/cites" href="http://rubensworks.net/raw/publications/2017/vtpf.pdf"><span class='abbreviation' title='Versioned Triple Pattern Fragments'>VTPF</span></a> <span class="references">[<a href="#ref-10">10</a>]</span> interface feature, <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives can be queried on the Web at a low cost,
as demonstrated on <a href="http://versioned.linkeddatafragments.org/bear" class="mandatory" data-link-text="http:/​/​versioned.linkeddatafragments.org/​bear">our public <span class='abbreviation' title='Versioned Triple Pattern Fragments'>VTPF</span> entrypoint</a>.
<span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> only requires triple pattern indexes with count metadata,
which means that <span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> clients are able to evaluate full <span class='abbreviation' title='Version materialization'>VM</span> <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries using <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and <span class='abbreviation' title='Versioned Triple Pattern Fragments'>VTPF</span>.
In future work, the <span class='abbreviation' title='Triple Pattern Fragments'>TPF</span> client will be extended to also support <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries.</p>

        <p>With <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, we provide a technique for publishing and querying <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives at Web-scale.
Several opportunities exist for advancing this technique in future work,
such as improving the ingestion efficiency, increasing the <span class='abbreviation' title='Delta materialization'>DM</span> offset efficiency,
and supporting dynamic snapshot creation.
Solutions could be based on existing
<a property="schema:citation http://purl.org/spar/cito/cites" href="http://users.ics.forth.gr/~fgeo/files/ER14.pdf">cost models</a> <span class="references">[<a href="#ref-52">52</a>]</span> for determining whether a new snapshot or delta
should be created based on quantified time and space parameters.
Furthermore, branching and merging of different version chains can be investigated.</p>

        <p>Our approach succeeds in reducing the cost for publishing <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives on the Web.
This lowers the barrier towards intelligent clients in the Semantic Web <span class="references">[<a href="#ref-53">53</a>]</span> that require <em>evolving</em> data,
with the goal of time-sensitive querying over the ever-evolving Web of data.</p>

      </section>

</main>

<footer>
  <section id="acknowledgements">
        <h2>Acknowledgements</h2>

        <p>We would like to thank Christophe Billiet for providing his insights into temporal databases.
We thank Giorgos Flouris for his comments on the structure and contents of this article,
and Javier D. Fernández for his help in setting up and running the <span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span> benchmark.
The described research activities were funded by Ghent University, imec,
Flanders Innovation &amp; Entrepreneurship (AIO), and the European Union.
Ruben Verborgh is a postdoctoral fellow of the Research Foundation – Flanders.</p>

      </section>

<section id="references">
<h2>References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="http://ceur-ws.org/Vol-1377/paper6.pdf" typeof="schema:Article">Fernández, J.D., Polleres, A., Umbrich, J.: Towards efficient archiving of Dynamic Linked Open Data. In: Debattista, J., d’Aquin, M., and Lange, C. (eds.) Proceedings of te First DIACHRON Workshop on Managing the Evolution and Preservation of the Data Web. pp. 34–49 (2015).</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="#linkeddata" typeof="schema:Article">Bizer, C., Heath, T., Berners-Lee, T.: Linked Data - the story so far. Semantic Services, Interoperability and Web Applications: Emerging Concepts. 205–227 (2009).</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/" typeof="schema:CreativeWork">Cyganiak, R., Wood, D., Lanthaler, M.: <span class='abbreviation' title='Resource Description Framework'>RDF</span> 1.1: Concepts and Abstract Syntax. W3C, <a href="http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/">http:/​/​www.w3.org/TR/2014/REC-rdf11-concepts-20140225/</a> (2014).</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="#datasetdynamics" typeof="schema:Article">Umbrich, J., Decker, S., Hausenblas, M., Polleres, A., Hogan, A.: Towards dataset dynamics: Change frequency of Linked Open Data sources. 3rd International Workshop on Linked Data on the Web (LDOW). (2010).</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="#diachronql" typeof="schema:Article">Meimaris, M., Papastefanatos, G., Viglas, S., Stavrakas, Y., Pateritsas, C., Anagnostopoulos, I.: A Query Language for Multi-version Data Web Archives. Expert Systems. 33, 383–404 (2016).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="#dbpedia" typeof="schema:Chapter">Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., Ives, Z.: DBpedia: A nucleus for a Web of open data. In: The semantic web. pp. 722–735. Springer (2007).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/" typeof="schema:CreativeWork">Harris, S., Seaborne, A., Prud’hommeaux, E.: <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> 1.1 Query Language. W3C, <a href="http://www.w3.org/TR/2013/REC-sparql11-query-20130321/">http:/​/​www.w3.org/TR/2013/REC-sparql11-query-20130321/</a> (2013).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="https://dx.doi.org/doi:10.1016/j.websem.2016.03.003" typeof="schema:Article">Verborgh, R., Vander Sande, M., Hartig, O., Van Herwegen, J., De Vocht, L., De Meester, B., Haesendonck, G., Colpaert, P.: Triple Pattern Fragments: a Low-cost Knowledge Graph Interface for the Web. Journal of Web Semantics. 37–38, (2016).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="#rdf3x" typeof="schema:Article">Neumann, T., Weikum, G.: <span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span>: a RISC-style engine for <span class='abbreviation' title='Resource Description Framework'>RDF</span>. Proceedings of the VLDB Endowment. 1, 647–659 (2008).</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2017/vtpf.pdf" typeof="schema:Article">Taelman, R., Vander Sande, M., Verborgh, R., Mannens, E.: Versioned Triple Pattern Fragments: A Low-cost Linked Data Interface Feature for Web Archives. In: Proceedings of the 3rd Workshop on Managing the Evolution and Preservation of the Data Web (2017).</dd>
  <dt id="ref-11">[11]</dt>
  <dd resource="http://linkeddatafragments.org/publications/jod2017.pdf" typeof="schema:Article">Vander Sande, M., Verborgh, R., Hochstenbach, P., Van de Sompel, H.: Towards sustainable publishing and querying of distributed Linked Data archives. Journal of Documentation. 73, (2017).</dd>
  <dt id="ref-12">[12]</dt>
  <dd resource="#memento" typeof="schema:Article">Van de Sompel, H., Nelson, M.L., Sanderson, R., Balakireva, L.L., Ainsworth, S., Shankar, H.: Memento: Time travel for the Web. arXiv preprint arXiv:0911.1112. (2009).</dd>
  <dt id="ref-13">[13]</dt>
  <dd resource="#virtuoso" typeof="schema:Chapter">Erling, O., Mikhailov, I.: Virtuoso: <span class='abbreviation' title='Resource Description Framework'>RDF</span> support in a native RDBMS. In: Semantic Web Information Management. pp. 501–519. Springer (2010).</dd>
  <dt id="ref-14">[14]</dt>
  <dd resource="#dsparq" typeof="schema:Article">Wallgrün, J.O., Frommberger, L., Wolter, D., Dylla, F., Freksa, C.: Qualitative spatial representation and reasoning in the SparQ-toolbox. In: International Conference on Spatial Cognition. pp. 39–58. Springer (2006).</dd>
  <dt id="ref-15">[15]</dt>
  <dd resource="#derivingemergentschemas" typeof="schema:Article">Pham, M.-D., Passing, L., Erling, O., Boncz, P.: Deriving an emergent relational schema from rdf data. In: Proceedings of the 24th International Conference on World Wide Web. pp. 864–874. International World Wide Web Conferences Steering Committee (2015).</dd>
  <dt id="ref-16">[16]</dt>
  <dd resource="#emergentschemas" typeof="schema:Article">Pham, M.-D., Boncz, P.: Exploiting emergent schemas to make <span class='abbreviation' title='Resource Description Framework'>RDF</span> systems more efficient. In: International Semantic Web Conference. pp. 463–479. Springer (2016).</dd>
  <dt id="ref-17">[17]</dt>
  <dd resource="#axondb" typeof="schema:Article">Meimaris, M., Papastefanatos, G., Mamoulis, N., Anagnostopoulos, I.: Extended characteristic sets: graph indexing for <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> query optimization. In: Data Engineering (ICDE), 2017 IEEE 33rd International Conference on. pp. 497–508. IEEE (2017).</dd>
  <dt id="ref-18">[18]</dt>
  <dd resource="#odyssey" typeof="schema:Article">Montoya, G., Skaf-Molli, H., Hose, K.: The Odyssey approach for optimizing federated <span class='abbreviation' title='SPARQL Procotol and RDF Query Language'>SPARQL</span> queries. In: International Semantic Web Conference. pp. 471–489. Springer (2017).</dd>
  <dt id="ref-19">[19]</dt>
  <dd resource="#hexastore" typeof="schema:Article">Weiss, C., Karras, P., Bernstein, A.: Hexastore: sextuple indexing for semantic web data management. Proceedings of the VLDB Endowment. 1, 1008–1019 (2008).</dd>
  <dt id="ref-20">[20]</dt>
  <dd resource="#triplebit" typeof="schema:Article">Yuan, P., Liu, P., Wu, B., Jin, H., Zhang, W., Liu, L.: TripleBit: a fast and compact system for large scale <span class='abbreviation' title='Resource Description Framework'>RDF</span> data. Proceedings of the VLDB Endowment. 6, 517–528 (2013).</dd>
  <dt id="ref-21">[21]</dt>
  <dd resource="https://arxiv.org/pdf/1105.4004.pdf" typeof="schema:Article">Álvarez-Garcı́a Sandra, Brisaboa, N.R., Fernández, J.D., Martı́nez-Prieto Miguel A: Compressed k2-triples for full-in-memory <span class='abbreviation' title='Resource Description Framework'>RDF</span> engines. arXiv preprint arXiv:1105.4004. (2011).</dd>
  <dt id="ref-22">[22]</dt>
  <dd resource="#rdfcsa" typeof="schema:Article">Brisaboa, N.R., Cerdeira-Pena, A., Fariña, A., Navarro, G.: A compact <span class='abbreviation' title='Resource Description Framework'>RDF</span> store using suffix arrays. In: International Symposium on String Processing and Information Retrieval. pp. 103–115. Springer (2015).</dd>
  <dt id="ref-23">[23]</dt>
  <dd resource="http://www.websemanticsjournal.org/index.php/ps/article/view/328" typeof="schema:Article">Fernández, J.D., Martínez-Prieto, M.A., Gutiérrez, C., Polleres, A., Arias, M.: Binary <span class='abbreviation' title='Resource Description Framework'>RDF</span> Representation for Publication and Exchange (<span class='abbreviation' title='Header Dictionary Triples'>HDT</span>). Web Semantics: Science, Services and Agents on the World Wide Web. 19, 22–41 (2013).</dd>
  <dt id="ref-24">[24]</dt>
  <dd resource="#hdtfoq" typeof="schema:Article">Martı́nez-Prieto Miguel A, Gallego, M.A., Fernández, J.D.: Exchange and consumption of huge <span class='abbreviation' title='Resource Description Framework'>RDF</span> data. In: Extended Semantic Web Conference. pp. 437–452. Springer (2012).</dd>
  <dt id="ref-25">[25]</dt>
  <dd resource="#lodlaundromat" typeof="schema:Article">Beek, W., Rietveld, L., Bazoobandi, H.R., Wielemaker, J., Schlobach, S.: LOD laundromat: a uniform way of publishing other people’s dirty data. In: International Semantic Web Conference. pp. 213–228. Springer (2014).</dd>
  <dt id="ref-26">[26]</dt>
  <dd resource="https://github.com/datablend/fluxgraph" typeof="schema:CreativeWork">Suvee, D.: fluxgraph. <a href="https://github.com/datablend/fluxgraph">https:/​/​github.com/datablend/fluxgraph</a> (2012).</dd>
  <dt id="ref-27">[27]</dt>
  <dd resource="https://github.com/dmontag/neo4j-versioning" typeof="schema:CreativeWork">Montag, D.: Neo4j Versioning. <a href="https://github.com/dmontag/neo4j-versioning">https:/​/​github.com/dmontag/neo4j-versioning</a> (2011).</dd>
  <dt id="ref-28">[28]</dt>
  <dd resource="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j" typeof="schema:CreativeWork">Cattuto, C.: Representing time dependent graphs in Neo4j. <a href="https://github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j">https:/​/​github.com/SocioPatterns/neo4j-dynagraph/wiki/Representing-time-dependent-graphs-in-Neo4j</a> (2013).</dd>
  <dt id="ref-29">[29]</dt>
  <dd resource="https://neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/" typeof="schema:CreativeWork">Staff, N.: Modeling a multilevel index in Neoj4. <a href="https://neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/">https:/​/​neo4j.com/blog/modeling-a-multilevel-index-in-neoj4/</a> (2012).</dd>
  <dt id="ref-30">[30]</dt>
  <dd resource="http://semantic-web-journal.org/system/files/swj1814.pdf" typeof="schema:Article">Fernández, J.D., Umbrich, J., Polleres, A., Knuth, M.: Evaluating Query and Storage Strategies for <span class='abbreviation' title='Resource Description Framework'>RDF</span> Archives. Semantic Web Journal. (2018).</dd>
  <dt id="ref-31">[31]</dt>
  <dd resource="#semversion" typeof="schema:Article">Volkel, M., Winkler, W., Sure, Y., Kruk, S.R., Synak, M.: Semversion: A versioning system for <span class='abbreviation' title='Resource Description Framework'>RDF</span> and ontologies. In: Second European Semantic Web Conference, ESWC 2005, Heraklion, Crete, Greece, May 29–June 1, 2005. Proceedings (2005).</dd>
  <dt id="ref-32">[32]</dt>
  <dd resource="#vcrdf" typeof="schema:Article">Cassidy, S., Ballantine, J.: Version Control for <span class='abbreviation' title='Resource Description Framework'>RDF</span> Triple Stores. ICSOFT (ISDM/EHST/DC). 7, 5–12 (2007).</dd>
  <dt id="ref-33">[33]</dt>
  <dd resource="#rwbase" typeof="schema:Article">Vander Sande, M., Colpaert, P., Verborgh, R., Coppens, S., Mannens, E., Van de Walle, R.: R&amp;Wbase: git for triples. In: Proceedings of the 6th Workshop on Linked Data on the Web (2013).</dd>
  <dt id="ref-34">[34]</dt>
  <dd resource="#r43ples" typeof="schema:Article">Graube, M., Hensel, S., Urbas, L.: R43ples: Revisions for triples. In: Proceedings of the 1st Workshop on Linked Data Quality co-located with 10th International Conference on Semantic Systems (SEMANTiCS 2014) (2014).</dd>
  <dt id="ref-35">[35]</dt>
  <dd resource="#vcld" typeof="schema:Article">Hauptmann, C., Brocco, M., Wörndl, W.: Scalable Semantic Version Control for Linked Data Management. In: Proceedings of the 2nd Workshop on Linked Data Quality co-located with 12th Extended Semantic Web Conference (ESWC 2015), Portorož, Slovenia (2015).</dd>
  <dt id="ref-36">[36]</dt>
  <dd resource="#xrdf3x" typeof="schema:Article">Neumann, T., Weikum, G.: x-<span class='abbreviation' title=''><span class='abbreviation' title='Resource Description Framework'>RDF</span>-3X</span>: fast querying, high update rates, and consistency for <span class='abbreviation' title='Resource Description Framework'>RDF</span> databases. Proceedings of the VLDB Endowment. 3, 256–263 (2010).</dd>
  <dt id="ref-37">[37]</dt>
  <dd resource="https://pdfs.semanticscholar.org/8efc/acc920a6329bda5508c65c84d69f52eb5ac1.pdf" typeof="schema:Article">Gao, S., Gu, J., Zaniolo, C.: <span class='abbreviation' title='Resource Description Framework'>RDF</span>-TX: A fast, user-friendly system for querying the history of <span class='abbreviation' title='Resource Description Framework'>RDF</span> knowledge bases. In: Proceedings of the 19th International Conference on Extending DatabaseTechnology. pp. 269–280 (2016).</dd>
  <dt id="ref-38">[38]</dt>
  <dd resource="#selfindexingarchives" typeof="schema:Article">Cerdeira-Pena, A., Farina, A., Fernández, J.D., Martı́nez-Prieto Miguel A: Self-indexing <span class='abbreviation' title='Resource Description Framework'>RDF</span> archives. In: Data Compression Conference (DCC), 2016. pp. 526–535. IEEE (2016).</dd>
  <dt id="ref-39">[39]</dt>
  <dd resource="#dydra" typeof="schema:Article">Anderson, J., Bendiken, A.: Transaction-time queries in dydra. In: Joint Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web (MEPDaW 2016) and the 3rd Workshop on Linked Data Quality (LDQ 2016) co-located with 13th European Semantic Web Conference (ESWC 2016): MEPDaW-LDQ. pp. 11–19 (2016).</dd>
  <dt id="ref-40">[40]</dt>
  <dd resource="#tailr" typeof="schema:Article">Meinhardt, P., Knuth, M., Sack, H.: TailR: a platform for preserving history on the web of data. In: Proceedings of the 11th International Conference on Semantic Systems. pp. 57–64. ACM (2015).</dd>
  <dt id="ref-41">[41]</dt>
  <dd resource="http://darcs.net" typeof="schema:CreativeWork">Roundy, D.: Darcs. <a href="http://darcs.net">http:/​/​darcs.net</a> (2008).</dd>
  <dt id="ref-42">[42]</dt>
  <dd resource="https://www.w3.org/TR/trig/" typeof="schema:CreativeWork">Bizer, C., Cyganiak, R.: <span class='abbreviation' title='Resource Description Framework'>RDF</span> 1.1 TriG. World Wide Web Consortium, <a href="https://www.w3.org/TR/trig/">https:/​/​www.w3.org/TR/trig/</a> (2014).</dd>
  <dt id="ref-43">[43]</dt>
  <dd resource="#vmrdf" typeof="schema:Article">Im, D.-H., Lee, S.-W., Kim, H.-J.: A version management framework for <span class='abbreviation' title='Resource Description Framework'>RDF</span> triple stores. International Journal of Software Engineering and Knowledge Engineering. 22, 85–106 (2012).</dd>
  <dt id="ref-44">[44]</dt>
  <dd resource="#sesame" typeof="schema:Article">Broekstra, J., Kampman, A., Van Harmelen, F.: Sesame: A generic architecture for storing and querying <span class='abbreviation' title='Resource Description Framework'>RDF</span> and <span class='abbreviation' title='Resource Description Framework'>RDF</span> schema. In: International semantic web conference. pp. 54–68. Springer (2002).</dd>
  <dt id="ref-45">[45]</dt>
  <dd resource="#blazegraph" typeof="schema:Article">Thompson, B.B., Personick, M., Cutcher, M.: The Bigdata® <span class='abbreviation' title='Resource Description Framework'>RDF</span> Graph Database. Linked Data Management. 193–237 (2014).</dd>
  <dt id="ref-46">[46]</dt>
  <dd resource="#dbpedialive" typeof="schema:Article">Morsey, M., Lehmann, J., Auer, S., Stadler, C., Hellmann, S.: DBpedia and the live extraction of structured data from wikipedia. Program. 46, 157–181 (2012).</dd>
  <dt id="ref-47">[47]</dt>
  <dd resource="#opendataportalwatch" typeof="schema:Article">Neumaier, S., Umbrich, J., Polleres, A.: Automated quality assessment of metadata across open data portals. Journal of Data and Information Quality (JDIQ). 8, 2 (2016).</dd>
  <dt id="ref-48">[48]</dt>
  <dd resource="#jena" typeof="schema:Article">McBride, B.: Jena: A semantic web toolkit. IEEE Internet computing. 6, 55–59 (2002).</dd>
  <dt id="ref-49">[49]</dt>
  <dd resource="#evogen" typeof="schema:Article">Meimaris, M., Papastefanatos, G.: The EvoGen Benchmark Suite for Evolving <span class='abbreviation' title='Resource Description Framework'>RDF</span> Data. In: Proceedings of the 2nd Workshop on Managing the Evolution and Preservation of the Data Web. pp. 20–35 (2016).</dd>
  <dt id="ref-50">[50]</dt>
  <dd resource="#lubm" typeof="schema:Article">Guo, Y., Pan, Z., Heflin, J.: <span class='abbreviation' title='Lehigh University Benchmark'>LUBM</span>: A benchmark for OWL knowledge base systems. Web Semantics: Science, Services and Agents on the World Wide Web. 3, 158–182 (2005).</dd>
  <dt id="ref-51">[51]</dt>
  <dd resource="http://rubensworks.net/raw/publications/2016/ExposingRdfArchivesUsingTpf.pdf" typeof="schema:Article">Taelman, R., Verborgh, R., Mannens, E.: Exposing <span class='abbreviation' title='Resource Description Framework'>RDF</span> Archives using Triple Pattern Fragments. In: Proceedings of the 20th International Conference on Knowledge Engineering and Knowledge Management: Posters and Demos (2016).</dd>
  <dt id="ref-52">[52]</dt>
  <dd resource="http://users.ics.forth.gr/~fgeo/files/ER14.pdf" typeof="schema:Article">Stefanidis, K., Chrysakis, I., Flouris, G.: On Designing Archiving Policies for Evolving <span class='abbreviation' title='Resource Description Framework'>RDF</span> Datasets on the Web. In: International Conference on Conceptual Modeling. pp. 43–56. Springer (2014).</dd>
  <dt id="ref-53">[53]</dt>
  <dd resource="#semanticweb" typeof="schema:Article">Berners-Lee, T., Hendler, J., Lassila, O., others: The semantic web. Scientific American. 284, 28–37 (2001).</dd>
</dl>
</section>
</footer>

<br />
<br />

<div id="appendix">
      <h1 id="appendix">Appendix</h1>
      <section id="appendix-bear-a">
        <h2><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> Query Evaluation results</h2>

        <p>In this appendix section, we list all measured <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span> query evaluation durations.
Each figure contains the durations for each storage approach, being
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Independent copies'>IC</span>, Jena-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Timestamp-based'>TB</span> and Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span>.</p>

        <figure id="result_beara-vm-s-low">
<img src="img/query/result_beara-vm-s-low.svg" alt="[bear-a S?? low vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 23:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for S?? triple patterns with a low cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-s-high">
<img src="img/query/result_beara-vm-s-high.svg" alt="[bear-a S?? high vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 24:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for S?? triple patterns with a high cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-p-low">
<img src="img/query/result_beara-vm-p-low.svg" alt="[bear-a ?P? low vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 25:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?P? triple patterns with a low cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-p-high">
<img src="img/query/result_beara-vm-p-high.svg" alt="[bear-a ?P? high vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 26:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?P? triple patterns with a high cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-o-low">
<img src="img/query/result_beara-vm-o-low.svg" alt="[bear-a ??O low vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 27:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ??O triple patterns with a low cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-o-high">
<img src="img/query/result_beara-vm-o-high.svg" alt="[bear-a ??O high vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 28:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ??O triple patterns with a high cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-sp-low">
<img src="img/query/result_beara-vm-sp-low.svg" alt="[bear-a SP? low vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 29:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for SP? triple patterns with a low cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-sp-high">
<img src="img/query/result_beara-vm-sp-high.svg" alt="[bear-a SP? high vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 30:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for SP? triple patterns with a high cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-so-low">
<img src="img/query/result_beara-vm-so-low.svg" alt="[bear-a S?O low vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 31:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for S?O triple patterns with a low cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-po-low">
<img src="img/query/result_beara-vm-po-low.svg" alt="[bear-a ?PO low vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 32:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?PO triple patterns with a low cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-po-high">
<img src="img/query/result_beara-vm-po-high.svg" alt="[bear-a ?PO high vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 33:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?PO triple patterns with a high cardinality for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vm-spo">
<img src="img/query/result_beara-vm-spo.svg" alt="[bear-a SPO vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 34:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for SPO triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-s-low">
<img src="img/query/result_beara-dm-s-low.svg" alt="[bear-a S?? low dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 35:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for S?? triple patterns with a low cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-s-high">
<img src="img/query/result_beara-dm-s-high.svg" alt="[bear-a S?? high dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 36:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for S?? triple patterns with a high cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-p-low">
<img src="img/query/result_beara-dm-p-low.svg" alt="[bear-a ?P? low dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 37:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?P? triple patterns with a low cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-p-high">
<img src="img/query/result_beara-dm-p-high.svg" alt="[bear-a ?P? high dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 38:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?P? triple patterns with a high cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-o-low">
<img src="img/query/result_beara-dm-o-low.svg" alt="[bear-a ??O low dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 39:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ??O triple patterns with a low cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-o-high">
<img src="img/query/result_beara-dm-o-high.svg" alt="[bear-a ??O high dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 40:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ??O triple patterns with a high cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-sp-low">
<img src="img/query/result_beara-dm-sp-low.svg" alt="[bear-a SP? low dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 41:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for SP? triple patterns with a low cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-sp-high">
<img src="img/query/result_beara-dm-sp-high.svg" alt="[bear-a SP? high dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 42:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for SP? triple patterns with a high cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-so-low">
<img src="img/query/result_beara-dm-so-low.svg" alt="[bear-a S?O low dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 43:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for S?O triple patterns with a low cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-po-low">
<img src="img/query/result_beara-dm-po-low.svg" alt="[bear-a ?PO low dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 44:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?PO triple patterns with a low cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-po-high">
<img src="img/query/result_beara-dm-po-high.svg" alt="[bear-a ?PO high dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 45:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?PO triple patterns with a high cardinality from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-dm-spo">
<img src="img/query/result_beara-dm-spo.svg" alt="[bear-a SPO dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 46:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for SPO triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-s-low">
<img src="img/query/result_beara-vq-s-low.svg" alt="[bear-a S?? low vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 47:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for S?? triple patterns with a low cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-s-high">
<img src="img/query/result_beara-vq-s-high.svg" alt="[bear-a S?? high vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 48:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for S?? triple patterns with a high cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-p-low">
<img src="img/query/result_beara-vq-p-low.svg" alt="[bear-a ?P? low vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 49:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?P? triple patterns with a low cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-p-high">
<img src="img/query/result_beara-vq-p-high.svg" alt="[bear-a ?P? high vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 50:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?P? triple patterns with a high cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-o-low">
<img src="img/query/result_beara-vq-o-low.svg" alt="[bear-a ??O low vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 51:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ??O triple patterns with a low cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-o-high">
<img src="img/query/result_beara-vq-o-high.svg" alt="[bear-a ??O high vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 52:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ??O triple patterns with a high cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-sp-low">
<img src="img/query/result_beara-vq-sp-low.svg" alt="[bear-a SP? low vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 53:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for SP? triple patterns with a low cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-sp-high">
<img src="img/query/result_beara-vq-sp-high.svg" alt="[bear-a SP? high vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 54:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for SP? triple patterns with a high cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-so-low">
<img src="img/query/result_beara-vq-so-low.svg" alt="[bear-a S?O low vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 55:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for S?O triple patterns with a low cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-po-low">
<img src="img/query/result_beara-vq-po-low.svg" alt="[bear-a ?PO low vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 56:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?PO triple patterns with a low cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-po-high">
<img src="img/query/result_beara-vq-po-high.svg" alt="[bear-a ?PO high vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 57:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?PO triple patterns with a high cardinality.</p>
          </figcaption>
</figure>

        <figure id="result_beara-vq-spo">
<img src="img/query/result_beara-vq-spo.svg" alt="[bear-a SPO vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 58:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for SPO triple patterns.</p>
          </figcaption>
</figure>
      </section>

      <section id="appendix-bear-b-daily">
        <h2><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily Query Evaluation results</h2>

        <p>In this appendix section, we list all measured <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily query evaluation durations.
Each figure contains the durations for each storage approach, being
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Independent copies'>IC</span>, Jena-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Timestamp-based'>TB</span> and Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span>.</p>

        <figure id="result_bearb-daily-vm-p">
<img src="img/query/result_bearb-daily-vm-p.svg" alt="[bear-b-daily ?P? vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 59:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?P? triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-daily-vm-po">
<img src="img/query/result_bearb-daily-vm-po.svg" alt="[bear-b-daily ?PO vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 60:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?PO triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-daily-dm-p">
<img src="img/query/result_bearb-daily-dm-p.svg" alt="[bear-b-daily ?P? dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 61:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?P? triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-daily-dm-po">
<img src="img/query/result_bearb-daily-dm-po.svg" alt="[bear-b-daily ?PO dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 62:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?PO triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-daily-vq-p">
<img src="img/query/result_bearb-daily-vq-p.svg" alt="[bear-b-daily ?P? vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 63:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?P? triple patterns.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-daily-vq-po">
<img src="img/query/result_bearb-daily-vq-po.svg" alt="[bear-b-daily ?PO vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 64:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?PO triple patterns.</p>
          </figcaption>
</figure>
      </section>

      <section id="appendix-bear-b-hourly">
        <h2><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly Query Evaluation results</h2>

        <p>In this appendix section, we list all measured <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly query evaluation durations.
Each figure contains the durations for each storage approach, being
<span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>, <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Independent copies'>IC</span>, Jena-<span class='abbreviation' title='Change-based'>CB</span>, Jena-<span class='abbreviation' title='Timestamp-based'>TB</span> and Jena-<span class='abbreviation' title='Change-based'>CB</span>/<span class='abbreviation' title='Timestamp-based'>TB</span>.</p>

        <figure id="result_bearb-hourly-vm-p">
<img src="img/query/result_bearb-hourly-vm-p.svg" alt="[bear-b-hourly ?P? vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 65:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?P? triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-hourly-vm-po">
<img src="img/query/result_bearb-hourly-vm-po.svg" alt="[bear-b-hourly ?PO vm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 66:</span> Average <span class='abbreviation' title='Version materialization'>VM</span> query results for ?PO triple patterns for all versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-hourly-dm-p">
<img src="img/query/result_bearb-hourly-dm-p.svg" alt="[bear-b-hourly ?P? dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 67:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?P? triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-hourly-dm-po">
<img src="img/query/result_bearb-hourly-dm-po.svg" alt="[bear-b-hourly ?PO dm]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 68:</span> Average <span class='abbreviation' title='Delta materialization'>DM</span> query results for ?PO triple patterns from version 0 to all other versions.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-hourly-vq-p">
<img src="img/query/result_bearb-hourly-vq-p.svg" alt="[bear-b-hourly ?P? vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 69:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?P? triple patterns.</p>
          </figcaption>
</figure>

        <figure id="result_bearb-hourly-vq-po">
<img src="img/query/result_bearb-hourly-vq-po.svg" alt="[bear-b-hourly ?PO vq]" height="200em" class="plot" />
<figcaption>
            <p><span class="label">Fig. 70:</span> Average <span class='abbreviation' title='Version query'>VQ</span> query results for ?PO triple patterns.</p>
          </figcaption>
</figure>
      </section>

      <section id="appendix-algorithms">
        <h2>Algorithms</h2>

        <p>In this appendix section, we show the different ingestion algorithms in pseudo-code.</p>

        <figure id="algorithm-ingestion-streaming" class="algorithm">
<pre><code>ingestStreaming(new, store)  {
</code><code>  newVersion = store.getLastVersion() + 1
</code><code>  additions = store.getAdditionsStream(store.getLastVersion())
</code><code>  deletions = store.getDeletionsStream(store.getLastVersion())
</code><code>  while (new.hasNext()
</code><code>      || additions.hasNext()
</code><code>      || deletions.hasNext()) {
</code><code>    if ( deletions.peek() &lt; new.peek()
</code><code>      &amp;&amp; deletions.peek() &lt; additions.peek()) {
</code><code>      deletion = deletions.next()
</code><code>      store.addDeletion(deletion.getTriple(), newVersion,
</code><code>        deletion.isLocalChange(), calculatePositions(deletion.getTriple()))
</code><code>    } else if (additions.peek() &lt; new.peek()
</code><code>            &amp;&amp; additions.peek() &lt; deletions.peek()) {  
</code><code>      addition = additions.next()
</code><code>      store.addAddition(addition.getTriple(),
</code><code>        newVersion, addition.isLocalChange())
</code><code>    } else if (new.peek() &lt; additions.peek()
</code><code>            &amp;&amp; new.peek() &lt; deletions.peek()) {
</code><code>      element = new.next()
</code><code>      if (element.isAddition()) {
</code><code>        store.addAddition(element.getTriple(), newVersion, false)
</code><code>      } else {
</code><code>        store.addDeletion(element.getTriple(), newVersion,
</code><code>          false, calculatePositions(element.getTriple()))
</code><code>      }
</code><code>    } else if (new.peek() = deletions.peek()
</code><code>            &amp;&amp; new.peek() &lt; additions.peek()) {
</code><code>      element = new.next()
</code><code>      deletion = deletions.next()
</code><code>      if (element.isAddition()) {
</code><code>        store.addAddition(element.getTriple(), newVersion, true)
</code><code>      } else {
</code><code>        store.addDeletion(element.getTriple(), newVersion,
</code><code>          deletion.isLocalChange(),
</code><code>          calculatePositions(element.getTriple()))
</code><code>      }
</code><code>    } else if (new.peek() = additions.peek()
</code><code>            &amp;&amp; new.peek() &lt; deletions.peek()) {
</code><code>      element = new.next()
</code><code>      addition = addition.next()
</code><code>      if (element.isAddition()) {
</code><code>        store.addAddition(element.getTriple(),
</code><code>          newVersion, addition.isLocalChange())
</code><code>      } else {
</code><code>        store.addDeletion(element.getTriple(), newVersion,
</code><code>          true, calculatePositions(element.getTriple()))
</code><code>      }
</code><code>    } else {
</code><code>      addition = addition.next()
</code><code>      deletion = deletions.next()
</code><code>      element = new.next()
</code><code>      if (addition &lt; element) {
</code><code>        if (addition.getLastVersion() &gt; deletion.getLastVersion()) {
</code><code>          store.addAddition(addition.getTriple(),
</code><code>            newVersion, addition.isLocalChange())
</code><code>        } else {
</code><code>          store.addDeletion(deletion.getTriple(), newVersion,
</code><code>            deletion.isLocalChange(),
</code><code>            calculatePositions(deletion.getTriple()))
</code><code>        }
</code><code>      } else {
</code><code>        if (addition.getLastVersion() &gt; deletion.getLastVersion()) {
</code><code>          store.addDeletion(element.getTriple(), newVersion,
</code><code>            addition.isLocalChange(),
</code><code>            calculatePositions(element.getTriple()))
</code><code>        } else {
</code><code>          store.addAddition(element.getTriple(),
</code><code>            newVersion, deletion.isLocalChange())
</code><code>        }
</code><code>      }
</code><code>    }
</code><code>  }
</code><code>}</code></pre>
<figcaption>
            <p><span class="label">Algorithm 3:</span> Streaming ingestion algorithm</p>
          </figcaption>
</figure>

        <figure id="algorithm-ingestion-batch" class="algorithm">
<pre><code>ingestBatch(changeset, store)  {
</code><code>  version   = store.getLastVersion() + 1
</code><code>  changeset = changeset
</code><code>      .read()
</code><code>      .sort()
</code><code>      .encode(store.getDictionary(newVersion))
</code><code>  lastChangeset   = store.getLastChangeset()
</code><code>  mergedChangeset = mergeChangesets(lastChangeset, changeset)
</code><code>  
</code><code>  for (triple, localChange : mergedChangeset.getAdditions()) {
</code><code>    store.addAddition(triple, version, localChange)
</code><code>  }
</code><code>  
</code><code>  for (triple, localChange : mergedChangeset.getDeletions()) {
</code><code>    positions = calculatePositions(triple)
</code><code>    store.addDeletion(triple, version, localChange, positions)
</code><code>  }
</code><code>}
</code></pre>
<figcaption>
            <p><span class="label">Algorithm 4:</span> Batch ingestion algorithm</p>
          </figcaption>
</figure>

      </section>

      <section id="appendix-tests">
        <h2>Statistical Tests</h2>

        <p>This appendix section contains the p-values for the different statistical tests about our hypotheses.</p>

        <figure id="hypo-test-1" class="table">

          <table>
            <thead>
              <tr>
                <th>Dataset</th>
                <th style="text-align: left">Query</th>
                <th style="text-align: left">Version (p)</th>
                <th>Results (p)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left"><strong>0.960</strong></td>
                <td><strong>0.570</strong></td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left"><strong>0.301</strong></td>
                <td><strong>0.320</strong></td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left"><strong>0.694</strong></td>
                <td><strong>0.697</strong></td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left"><strong>0.0391</strong></td>
                <td>2.13e-09</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left"><strong>0.568319</strong></td>
                <td>0.000574</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left"><strong>0.259</strong></td>
                <td>2e-16</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 11:</span> P-values for the linear regression model factors with the lookup time as response,
and version and number of results as factors for each of the three benchmarks for <span class='abbreviation' title='Version materialization'>VM</span> and <span class='abbreviation' title='Delta materialization'>DM</span> queries.
For all cases, the version factor has no significant influence on the models,
the number of results has an influence in the last three cases.</p>
          </figcaption>
        </figure>

        <figure id="hypo-test-2" class="table">

          <table>
            <thead>
              <tr>
                <th>Dataset</th>
                <th style="text-align: left">Query</th>
                <th style="text-align: left">p</th>
                <th>O ≤ H</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left">1.387e-05</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Version query'>VQ</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Version query'>VQ</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Version query'>VQ</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 12:</span> P-values for the two-sample t-test for testing equal means between <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span> lookup times
for <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries in <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
The last column indicates whether or not the actual lookup time mean of <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is less than or equal to <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Independent copies'>IC</span>.</p>
          </figcaption>
        </figure>

        <figure id="hypo-test-3" class="table">

          <table>
            <thead>
              <tr>
                <th>Dataset</th>
                <th style="text-align: left">Query</th>
                <th style="text-align: left">p</th>
                <th>O ≤ H</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left">1.68e-05</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span></td>
                <td style="text-align: left"><span class='abbreviation' title='Version query'>VQ</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✕</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left"><strong>0.02863</strong></td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily</td>
                <td style="text-align: left"><span class='abbreviation' title='Version query'>VQ</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Version materialization'>VM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Delta materialization'>DM</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
              <tr>
                <td><span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly</td>
                <td style="text-align: left"><span class='abbreviation' title='Version query'>VQ</span></td>
                <td style="text-align: left">&lt; 2.2e-16</td>
                <td>✓</td>
              </tr>
            </tbody>
          </table>

          <figcaption>
            <p><span class="label">Table 13:</span> P-values for the two-sample t-test for testing equal means between <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> and <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span> lookup times
for <span class='abbreviation' title='Version materialization'>VM</span>, <span class='abbreviation' title='Delta materialization'>DM</span> and <span class='abbreviation' title='Version query'>VQ</span> queries in <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-A</span>, <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-daily and <span class='abbreviation' title=''><span class='abbreviation' title='Benchmark of rdf Archives'>BEAR</span>-B</span>-hourly.
The last column indicates whether or not the actual lookup time mean of <span class='abbreviation' title='Offset-Enabled Triple Store for Changesets'>OSTRICH</span> is less than or equal to <span class='abbreviation' title='Header Dictionary Triples'>HDT</span>-<span class='abbreviation' title='Change-based'>CB</span>.</p>
          </figcaption>
        </figure>
      </section>

    </div>

</div>
</div>



</body>
</html>
